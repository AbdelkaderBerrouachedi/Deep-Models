{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt \n",
    "from pprint import pprint\n",
    "from numpy import array\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LB</th>\n",
       "      <th>AC</th>\n",
       "      <th>FM</th>\n",
       "      <th>UC</th>\n",
       "      <th>ASTV</th>\n",
       "      <th>MSTV</th>\n",
       "      <th>ALTV</th>\n",
       "      <th>MLTV</th>\n",
       "      <th>DL</th>\n",
       "      <th>DS</th>\n",
       "      <th>...</th>\n",
       "      <th>Min</th>\n",
       "      <th>Max</th>\n",
       "      <th>Nmax</th>\n",
       "      <th>Nzeros</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Median</th>\n",
       "      <th>Variance</th>\n",
       "      <th>Tendency</th>\n",
       "      <th>outlier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>146.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>33.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>134.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>79.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>114.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>149.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>34.0</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>148.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>153.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>122.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>62.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>134.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>42.0</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>128.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      LB   AC   FM   UC  ASTV  MSTV  ALTV  MLTV   DL   DS   ...       Min  \\\n",
       "0  146.0  0.0  0.0  5.0  65.0   0.4  33.0   7.4  0.0  0.0   ...     134.0   \n",
       "1  128.0  0.0  0.0  2.0  86.0   0.3  79.0   2.9  0.0  0.0   ...     114.0   \n",
       "2  149.0  0.0  0.0  5.0  61.0   0.4  34.0   5.6  0.0  0.0   ...     148.0   \n",
       "3  122.0  0.0  0.0  0.0  83.0   0.5   6.0  15.6  0.0  0.0   ...      62.0   \n",
       "4  134.0  0.0  4.0  0.0  79.0   0.2  42.0   5.5  0.0  0.0   ...     128.0   \n",
       "\n",
       "     Max  Nmax  Nzeros   Mode   Mean  Median  Variance  Tendency  outlier  \n",
       "0  164.0   1.0     0.0  150.0  149.0   151.0       1.0       0.0        1  \n",
       "1  130.0   0.0     0.0  128.0  126.0   129.0       0.0       1.0        1  \n",
       "2  160.0   1.0     0.0  154.0  153.0   155.0       0.0       0.0        1  \n",
       "3  130.0   0.0     0.0  122.0  122.0   123.0       3.0       1.0        1  \n",
       "4  145.0   2.0     0.0  135.0  135.0   136.0       1.0       0.0        1  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "df=pd.read_csv('Cardiotocography_02_v10.csv')  \n",
    "\n",
    "del df['id']\n",
    "del df['Unnamed: 0']\n",
    "df['outlier'] = df.outlier.apply(lambda label: 1 if label == \"'yes'\" else 0)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1688, 22)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 5 \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df to values\n",
    "df = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GC Forest\n",
    "import argparse\n",
    "import sys\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score\n",
    "sys.path.insert(0, \"lib\")\n",
    "from gcforest.gcforest import GCForest\n",
    "from gcforest.gcforest import GCForest\n",
    "from gcforest.utils.config_utils import load_json\n",
    "config = load_json(\"./examples/Cardiotocography.json\")   \n",
    "gc = GCForest(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# train test \n",
    "from sklearn.cross_validation import train_test_split\n",
    "y = df[:,21]\n",
    "X = df[:,0:21]\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of class\n",
    "len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2018-07-31 16:52:05,225][cascade_classifier.fit_transform] X_groups_train.shape=[(1181, 21)],y_train.shape=(1181,),X_groups_test.shape=[(507, 21)],y_test.shape=(507,)\n",
      "[ 2018-07-31 16:52:05,227][cascade_classifier.fit_transform] group_dims=[21]\n",
      "[ 2018-07-31 16:52:05,228][cascade_classifier.fit_transform] group_starts=[0]\n",
      "[ 2018-07-31 16:52:05,234][cascade_classifier.fit_transform] group_ends=[21]\n",
      "[ 2018-07-31 16:52:05,236][cascade_classifier.fit_transform] X_train.shape=(1181, 21),X_test.shape=(507, 21)\n",
      "[ 2018-07-31 16:52:05,237][cascade_classifier.fit_transform] [layer=0] look_indexs=[0], X_cur_train.shape=(1181, 21), X_cur_test.shape=(507, 21)\n",
      "[ 2018-07-31 16:52:05,959][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_0.predict)=98.32%\n",
      "[ 2018-07-31 16:52:07,114][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_1.predict)=98.31%\n",
      "[ 2018-07-31 16:52:07,921][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_2.predict)=99.15%\n",
      "[ 2018-07-31 16:52:08,539][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_3.predict)=98.31%\n",
      "[ 2018-07-31 16:52:09,273][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_4.predict)=99.15%\n",
      "[ 2018-07-31 16:52:10,009][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_5.predict)=98.31%\n",
      "[ 2018-07-31 16:52:10,630][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_6.predict)=98.31%\n",
      "[ 2018-07-31 16:52:11,378][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_7.predict)=98.31%\n",
      "[ 2018-07-31 16:52:11,999][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_8.predict)=98.31%\n",
      "[ 2018-07-31 16:52:12,914][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_9.predict)=99.15%\n",
      "[ 2018-07-31 16:52:13,136][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_cv.predict)=98.56%\n",
      "[ 2018-07-31 16:52:13,137][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.test.predict)=97.44%\n",
      "[ 2018-07-31 16:52:13,632][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_0.predict)=98.32%\n",
      "[ 2018-07-31 16:52:14,231][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_1.predict)=98.31%\n",
      "[ 2018-07-31 16:52:14,944][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_2.predict)=98.31%\n",
      "[ 2018-07-31 16:52:15,551][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_3.predict)=98.31%\n",
      "[ 2018-07-31 16:52:16,266][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_4.predict)=98.31%\n",
      "[ 2018-07-31 16:52:17,130][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_5.predict)=98.31%\n",
      "[ 2018-07-31 16:52:18,069][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_6.predict)=98.31%\n",
      "[ 2018-07-31 16:52:18,819][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_7.predict)=99.15%\n",
      "[ 2018-07-31 16:52:19,524][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_8.predict)=98.31%\n",
      "[ 2018-07-31 16:52:20,250][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_9.predict)=98.31%\n",
      "[ 2018-07-31 16:52:20,472][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_cv.predict)=98.39%\n",
      "[ 2018-07-31 16:52:20,473][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.test.predict)=97.63%\n",
      "[ 2018-07-31 16:52:20,512][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_0.predict)=99.16%\n",
      "[ 2018-07-31 16:52:20,537][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_1.predict)=98.31%\n",
      "[ 2018-07-31 16:52:20,564][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_2.predict)=98.31%\n",
      "[ 2018-07-31 16:52:20,587][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_3.predict)=98.31%\n",
      "[ 2018-07-31 16:52:20,612][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_4.predict)=99.15%\n",
      "[ 2018-07-31 16:52:20,637][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_5.predict)=98.31%\n",
      "[ 2018-07-31 16:52:20,661][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_6.predict)=97.46%\n",
      "[ 2018-07-31 16:52:20,682][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_7.predict)=97.46%\n",
      "[ 2018-07-31 16:52:20,702][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_8.predict)=98.31%\n",
      "[ 2018-07-31 16:52:20,724][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_9.predict)=98.31%\n",
      "[ 2018-07-31 16:52:20,725][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_cv.predict)=98.31%\n",
      "[ 2018-07-31 16:52:20,726][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.test.predict)=97.83%\n",
      "[ 2018-07-31 16:52:20,727][cascade_classifier.calc_accuracy] Accuracy(layer_0 - train.classifier_average)=98.56%\n",
      "[ 2018-07-31 16:52:20,728][cascade_classifier.calc_accuracy] Accuracy(layer_0 - test.classifier_average)=97.63%\n",
      "[ 2018-07-31 16:52:20,729][cascade_classifier.fit_transform] [layer=1] look_indexs=[0], X_cur_train.shape=(1181, 27), X_cur_test.shape=(507, 27)\n",
      "[ 2018-07-31 16:52:21,259][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_0.predict)=99.16%\n",
      "[ 2018-07-31 16:52:22,129][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_1.predict)=99.15%\n",
      "[ 2018-07-31 16:52:23,119][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_2.predict)=99.15%\n",
      "[ 2018-07-31 16:52:24,249][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_3.predict)=99.15%\n",
      "[ 2018-07-31 16:52:25,267][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_4.predict)=98.31%\n",
      "[ 2018-07-31 16:52:26,208][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_5.predict)=98.31%\n",
      "[ 2018-07-31 16:52:26,931][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_6.predict)=98.31%\n",
      "[ 2018-07-31 16:52:27,798][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_7.predict)=98.31%\n",
      "[ 2018-07-31 16:52:28,555][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_8.predict)=98.31%\n",
      "[ 2018-07-31 16:52:29,505][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_9.predict)=98.31%\n",
      "[ 2018-07-31 16:52:29,743][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_cv.predict)=98.65%\n",
      "[ 2018-07-31 16:52:29,745][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.test.predict)=97.63%\n",
      "[ 2018-07-31 16:52:30,486][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_0.predict)=99.16%\n",
      "[ 2018-07-31 16:52:31,216][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_1.predict)=99.15%\n",
      "[ 2018-07-31 16:52:32,107][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_2.predict)=98.31%\n",
      "[ 2018-07-31 16:52:32,838][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_3.predict)=99.15%\n",
      "[ 2018-07-31 16:52:33,435][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_4.predict)=98.31%\n",
      "[ 2018-07-31 16:52:34,148][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_5.predict)=99.15%\n",
      "[ 2018-07-31 16:52:34,871][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_6.predict)=99.15%\n",
      "[ 2018-07-31 16:52:35,584][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_7.predict)=99.15%\n",
      "[ 2018-07-31 16:52:36,304][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_8.predict)=98.31%\n",
      "[ 2018-07-31 16:52:37,024][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_9.predict)=98.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2018-07-31 16:52:37,247][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_cv.predict)=98.81%\n",
      "[ 2018-07-31 16:52:37,248][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.test.predict)=97.63%\n",
      "[ 2018-07-31 16:52:37,281][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_0.predict)=98.32%\n",
      "[ 2018-07-31 16:52:37,311][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_1.predict)=98.31%\n",
      "[ 2018-07-31 16:52:37,339][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_2.predict)=98.31%\n",
      "[ 2018-07-31 16:52:37,370][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_3.predict)=98.31%\n",
      "[ 2018-07-31 16:52:37,405][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_4.predict)=98.31%\n",
      "[ 2018-07-31 16:52:37,432][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_5.predict)=98.31%\n",
      "[ 2018-07-31 16:52:37,463][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_6.predict)=98.31%\n",
      "[ 2018-07-31 16:52:37,495][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_7.predict)=99.15%\n",
      "[ 2018-07-31 16:52:37,526][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_8.predict)=99.15%\n",
      "[ 2018-07-31 16:52:37,553][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_9.predict)=98.31%\n",
      "[ 2018-07-31 16:52:37,554][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_cv.predict)=98.48%\n",
      "[ 2018-07-31 16:52:37,555][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.test.predict)=97.83%\n",
      "[ 2018-07-31 16:52:37,556][cascade_classifier.calc_accuracy] Accuracy(layer_1 - train.classifier_average)=98.56%\n",
      "[ 2018-07-31 16:52:37,557][cascade_classifier.calc_accuracy] Accuracy(layer_1 - test.classifier_average)=97.63%\n",
      "[ 2018-07-31 16:52:37,558][cascade_classifier.fit_transform] [layer=2] look_indexs=[0], X_cur_train.shape=(1181, 27), X_cur_test.shape=(507, 27)\n",
      "[ 2018-07-31 16:52:38,076][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_0.predict)=100.00%\n",
      "[ 2018-07-31 16:52:38,701][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_1.predict)=98.31%\n",
      "[ 2018-07-31 16:52:39,324][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_2.predict)=98.31%\n",
      "[ 2018-07-31 16:52:40,063][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_3.predict)=97.46%\n",
      "[ 2018-07-31 16:52:40,793][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_4.predict)=99.15%\n",
      "[ 2018-07-31 16:52:41,416][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_5.predict)=98.31%\n",
      "[ 2018-07-31 16:52:42,165][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_6.predict)=99.15%\n",
      "[ 2018-07-31 16:52:42,902][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_7.predict)=98.31%\n",
      "[ 2018-07-31 16:52:43,636][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_8.predict)=98.31%\n",
      "[ 2018-07-31 16:52:44,389][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_9.predict)=99.15%\n",
      "[ 2018-07-31 16:52:44,494][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_cv.predict)=98.65%\n",
      "[ 2018-07-31 16:52:44,496][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.test.predict)=97.63%\n",
      "[ 2018-07-31 16:52:44,992][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_0.predict)=97.48%\n",
      "[ 2018-07-31 16:52:45,713][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_1.predict)=99.15%\n",
      "[ 2018-07-31 16:52:46,436][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_2.predict)=98.31%\n",
      "[ 2018-07-31 16:52:47,037][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_3.predict)=99.15%\n",
      "[ 2018-07-31 16:52:47,748][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_4.predict)=98.31%\n",
      "[ 2018-07-31 16:52:48,460][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_5.predict)=99.15%\n",
      "[ 2018-07-31 16:52:49,292][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_6.predict)=99.15%\n",
      "[ 2018-07-31 16:52:50,014][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_7.predict)=98.31%\n",
      "[ 2018-07-31 16:52:50,743][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_8.predict)=98.31%\n",
      "[ 2018-07-31 16:52:51,599][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_9.predict)=99.15%\n",
      "[ 2018-07-31 16:52:51,824][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_cv.predict)=98.65%\n",
      "[ 2018-07-31 16:52:51,826][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.test.predict)=97.83%\n",
      "[ 2018-07-31 16:52:51,861][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_0.predict)=99.16%\n",
      "[ 2018-07-31 16:52:51,896][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_1.predict)=98.31%\n",
      "[ 2018-07-31 16:52:51,931][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_2.predict)=98.31%\n",
      "[ 2018-07-31 16:52:51,969][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_3.predict)=98.31%\n",
      "[ 2018-07-31 16:52:52,002][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_4.predict)=99.15%\n",
      "[ 2018-07-31 16:52:52,036][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_5.predict)=98.31%\n",
      "[ 2018-07-31 16:52:52,065][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_6.predict)=98.31%\n",
      "[ 2018-07-31 16:52:52,103][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_7.predict)=98.31%\n",
      "[ 2018-07-31 16:52:52,136][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_8.predict)=98.31%\n",
      "[ 2018-07-31 16:52:52,170][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_9.predict)=98.31%\n",
      "[ 2018-07-31 16:52:52,171][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_cv.predict)=98.48%\n",
      "[ 2018-07-31 16:52:52,172][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.test.predict)=97.63%\n",
      "[ 2018-07-31 16:52:52,173][cascade_classifier.calc_accuracy] Accuracy(layer_2 - train.classifier_average)=98.65%\n",
      "[ 2018-07-31 16:52:52,174][cascade_classifier.calc_accuracy] Accuracy(layer_2 - test.classifier_average)=97.83%\n",
      "[ 2018-07-31 16:52:52,176][cascade_classifier.fit_transform] [layer=3] look_indexs=[0], X_cur_train.shape=(1181, 27), X_cur_test.shape=(507, 27)\n",
      "[ 2018-07-31 16:52:52,822][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_0.predict)=99.16%\n",
      "[ 2018-07-31 16:52:53,588][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_1.predict)=99.15%\n",
      "[ 2018-07-31 16:52:54,445][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_2.predict)=98.31%\n",
      "[ 2018-07-31 16:52:55,359][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_3.predict)=99.15%\n",
      "[ 2018-07-31 16:52:56,233][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_4.predict)=98.31%\n",
      "[ 2018-07-31 16:52:57,068][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_5.predict)=98.31%\n",
      "[ 2018-07-31 16:52:57,980][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_6.predict)=96.61%\n",
      "[ 2018-07-31 16:52:58,599][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_7.predict)=98.31%\n",
      "[ 2018-07-31 16:52:59,222][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_8.predict)=99.15%\n",
      "[ 2018-07-31 16:52:59,960][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_9.predict)=99.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2018-07-31 16:53:00,067][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_cv.predict)=98.56%\n",
      "[ 2018-07-31 16:53:00,068][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.test.predict)=97.83%\n",
      "[ 2018-07-31 16:53:00,562][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_0.predict)=99.16%\n",
      "[ 2018-07-31 16:53:01,278][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_1.predict)=98.31%\n",
      "[ 2018-07-31 16:53:01,998][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_2.predict)=98.31%\n",
      "[ 2018-07-31 16:53:02,847][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_3.predict)=99.15%\n",
      "[ 2018-07-31 16:53:03,570][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_4.predict)=98.31%\n",
      "[ 2018-07-31 16:53:04,285][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_5.predict)=99.15%\n",
      "[ 2018-07-31 16:53:05,002][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_6.predict)=98.31%\n",
      "[ 2018-07-31 16:53:05,835][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_7.predict)=98.31%\n",
      "[ 2018-07-31 16:53:06,550][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_8.predict)=98.31%\n",
      "[ 2018-07-31 16:53:07,153][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_9.predict)=99.15%\n",
      "[ 2018-07-31 16:53:07,260][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_cv.predict)=98.65%\n",
      "[ 2018-07-31 16:53:07,261][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.test.predict)=97.63%\n",
      "[ 2018-07-31 16:53:07,296][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_0.predict)=98.32%\n",
      "[ 2018-07-31 16:53:07,327][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_1.predict)=99.15%\n",
      "[ 2018-07-31 16:53:07,363][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_2.predict)=98.31%\n",
      "[ 2018-07-31 16:53:07,395][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_3.predict)=98.31%\n",
      "[ 2018-07-31 16:53:07,426][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_4.predict)=97.46%\n",
      "[ 2018-07-31 16:53:07,461][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_5.predict)=98.31%\n",
      "[ 2018-07-31 16:53:07,496][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_6.predict)=99.15%\n",
      "[ 2018-07-31 16:53:07,525][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_7.predict)=99.15%\n",
      "[ 2018-07-31 16:53:07,556][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_8.predict)=97.46%\n",
      "[ 2018-07-31 16:53:07,591][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_9.predict)=99.15%\n",
      "[ 2018-07-31 16:53:07,592][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_cv.predict)=98.48%\n",
      "[ 2018-07-31 16:53:07,593][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.test.predict)=97.83%\n",
      "[ 2018-07-31 16:53:07,594][cascade_classifier.calc_accuracy] Accuracy(layer_3 - train.classifier_average)=98.48%\n",
      "[ 2018-07-31 16:53:07,595][cascade_classifier.calc_accuracy] Accuracy(layer_3 - test.classifier_average)=97.63%\n",
      "[ 2018-07-31 16:53:07,596][cascade_classifier.fit_transform] [layer=4] look_indexs=[0], X_cur_train.shape=(1181, 27), X_cur_test.shape=(507, 27)\n",
      "[ 2018-07-31 16:53:08,118][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_0.predict)=99.16%\n",
      "[ 2018-07-31 16:53:08,859][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_1.predict)=99.15%\n",
      "[ 2018-07-31 16:53:09,593][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_2.predict)=97.46%\n",
      "[ 2018-07-31 16:53:10,333][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_3.predict)=99.15%\n",
      "[ 2018-07-31 16:53:11,117][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_4.predict)=99.15%\n",
      "[ 2018-07-31 16:53:11,853][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_5.predict)=99.15%\n",
      "[ 2018-07-31 16:53:12,593][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_6.predict)=98.31%\n",
      "[ 2018-07-31 16:53:13,452][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_7.predict)=97.46%\n",
      "[ 2018-07-31 16:53:14,194][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_8.predict)=98.31%\n",
      "[ 2018-07-31 16:53:14,939][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_9.predict)=98.31%\n",
      "[ 2018-07-31 16:53:15,047][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_cv.predict)=98.56%\n",
      "[ 2018-07-31 16:53:15,048][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.test.predict)=97.63%\n",
      "[ 2018-07-31 16:53:15,544][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_0.predict)=98.32%\n",
      "[ 2018-07-31 16:53:16,395][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_1.predict)=98.31%\n",
      "[ 2018-07-31 16:53:17,114][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_2.predict)=98.31%\n",
      "[ 2018-07-31 16:53:17,724][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_3.predict)=98.31%\n",
      "[ 2018-07-31 16:53:18,437][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_4.predict)=100.00%\n",
      "[ 2018-07-31 16:53:19,153][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_5.predict)=99.15%\n",
      "[ 2018-07-31 16:53:19,982][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_6.predict)=97.46%\n",
      "[ 2018-07-31 16:53:20,698][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_7.predict)=99.15%\n",
      "[ 2018-07-31 16:53:21,297][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_8.predict)=98.31%\n",
      "[ 2018-07-31 16:53:22,019][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_9.predict)=99.15%\n",
      "[ 2018-07-31 16:53:22,125][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_cv.predict)=98.65%\n",
      "[ 2018-07-31 16:53:22,126][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.test.predict)=97.83%\n",
      "[ 2018-07-31 16:53:22,158][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_0.predict)=99.16%\n",
      "[ 2018-07-31 16:53:22,202][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_1.predict)=98.31%\n",
      "[ 2018-07-31 16:53:22,235][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_2.predict)=98.31%\n",
      "[ 2018-07-31 16:53:22,276][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_3.predict)=98.31%\n",
      "[ 2018-07-31 16:53:22,310][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_4.predict)=98.31%\n",
      "[ 2018-07-31 16:53:22,342][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_5.predict)=98.31%\n",
      "[ 2018-07-31 16:53:22,373][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_6.predict)=99.15%\n",
      "[ 2018-07-31 16:53:22,404][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_7.predict)=99.15%\n",
      "[ 2018-07-31 16:53:22,434][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_8.predict)=97.46%\n",
      "[ 2018-07-31 16:53:22,474][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_9.predict)=99.15%\n",
      "[ 2018-07-31 16:53:22,476][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_cv.predict)=98.56%\n",
      "[ 2018-07-31 16:53:22,477][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.test.predict)=97.63%\n",
      "[ 2018-07-31 16:53:22,477][cascade_classifier.calc_accuracy] Accuracy(layer_4 - train.classifier_average)=98.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2018-07-31 16:53:22,478][cascade_classifier.calc_accuracy] Accuracy(layer_4 - test.classifier_average)=97.63%\n",
      "[ 2018-07-31 16:53:22,479][cascade_classifier.fit_transform] [layer=5] look_indexs=[0], X_cur_train.shape=(1181, 27), X_cur_test.shape=(507, 27)\n",
      "[ 2018-07-31 16:53:23,124][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_0 - 10_folds.train_0.predict)=98.32%\n",
      "[ 2018-07-31 16:53:23,911][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_0 - 10_folds.train_1.predict)=99.15%\n",
      "[ 2018-07-31 16:53:24,532][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_0 - 10_folds.train_2.predict)=99.15%\n",
      "[ 2018-07-31 16:53:25,155][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_0 - 10_folds.train_3.predict)=98.31%\n",
      "[ 2018-07-31 16:53:25,893][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_0 - 10_folds.train_4.predict)=98.31%\n",
      "[ 2018-07-31 16:53:26,636][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_0 - 10_folds.train_5.predict)=98.31%\n",
      "[ 2018-07-31 16:53:27,367][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_0 - 10_folds.train_6.predict)=97.46%\n",
      "[ 2018-07-31 16:53:28,101][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_0 - 10_folds.train_7.predict)=98.31%\n",
      "[ 2018-07-31 16:53:28,839][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_0 - 10_folds.train_8.predict)=98.31%\n",
      "[ 2018-07-31 16:53:29,470][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_0 - 10_folds.train_9.predict)=99.15%\n",
      "[ 2018-07-31 16:53:29,691][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_0 - 10_folds.train_cv.predict)=98.48%\n",
      "[ 2018-07-31 16:53:29,692][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_0 - 10_folds.test.predict)=97.63%\n",
      "[ 2018-07-31 16:53:30,189][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_1 - 10_folds.train_0.predict)=98.32%\n",
      "[ 2018-07-31 16:53:31,030][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_1 - 10_folds.train_1.predict)=98.31%\n",
      "[ 2018-07-31 16:53:31,744][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_1 - 10_folds.train_2.predict)=98.31%\n",
      "[ 2018-07-31 16:53:32,460][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_1 - 10_folds.train_3.predict)=97.46%\n",
      "[ 2018-07-31 16:53:33,065][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_1 - 10_folds.train_4.predict)=99.15%\n",
      "[ 2018-07-31 16:53:33,779][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_1 - 10_folds.train_5.predict)=98.31%\n",
      "[ 2018-07-31 16:53:34,505][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_1 - 10_folds.train_6.predict)=98.31%\n",
      "[ 2018-07-31 16:53:35,227][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_1 - 10_folds.train_7.predict)=98.31%\n",
      "[ 2018-07-31 16:53:35,942][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_1 - 10_folds.train_8.predict)=98.31%\n",
      "[ 2018-07-31 16:53:36,710][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_1 - 10_folds.train_9.predict)=99.15%\n",
      "[ 2018-07-31 16:53:36,817][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_1 - 10_folds.train_cv.predict)=98.39%\n",
      "[ 2018-07-31 16:53:36,818][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_1 - 10_folds.test.predict)=97.83%\n",
      "[ 2018-07-31 16:53:36,853][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_2 - 10_folds.train_0.predict)=98.32%\n",
      "[ 2018-07-31 16:53:36,887][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_2 - 10_folds.train_1.predict)=97.46%\n",
      "[ 2018-07-31 16:53:36,918][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_2 - 10_folds.train_2.predict)=99.15%\n",
      "[ 2018-07-31 16:53:36,956][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_2 - 10_folds.train_3.predict)=97.46%\n",
      "[ 2018-07-31 16:53:36,989][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_2 - 10_folds.train_4.predict)=98.31%\n",
      "[ 2018-07-31 16:53:37,021][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_2 - 10_folds.train_5.predict)=98.31%\n",
      "[ 2018-07-31 16:53:37,056][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_2 - 10_folds.train_6.predict)=98.31%\n",
      "[ 2018-07-31 16:53:37,092][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_2 - 10_folds.train_7.predict)=98.31%\n",
      "[ 2018-07-31 16:53:37,128][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_2 - 10_folds.train_8.predict)=98.31%\n",
      "[ 2018-07-31 16:53:37,164][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_2 - 10_folds.train_9.predict)=99.15%\n",
      "[ 2018-07-31 16:53:37,166][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_2 - 10_folds.train_cv.predict)=98.31%\n",
      "[ 2018-07-31 16:53:37,167][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_2 - 10_folds.test.predict)=97.83%\n",
      "[ 2018-07-31 16:53:37,168][cascade_classifier.calc_accuracy] Accuracy(layer_5 - train.classifier_average)=98.39%\n",
      "[ 2018-07-31 16:53:37,168][cascade_classifier.calc_accuracy] Accuracy(layer_5 - test.classifier_average)=97.83%\n",
      "[ 2018-07-31 16:53:37,169][cascade_classifier.fit_transform] [Result][Optimal Level Detected] opt_layer_num=3, accuracy_train=98.65%, accuracy_test=97.83%\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "# X_enc is the concatenated predict_proba result of each estimators of the last layer of the GCForest model\n",
    "    # X_enc.shape =\n",
    "    #   (n_datas, n_estimators * n_classes): If cascade is provided\n",
    "    #   (n_datas, n_estimators * n_classes, dimX, dimY): If only finegrained part is provided\n",
    "    # You can also pass X_test, y_test to fit_transform method, then the accracy on test data will be logged when training.\n",
    "X_train_enc, X_test_enc = gc.fit_transform(X_train, y_train, X_test=X_test, y_test=y_test)\n",
    "    # WARNING: if you set gc.set_keep_model_in_mem(True), you would have to use\n",
    "    # gc.fit_transform(X_train, y_train, X_test=X_test, y_test=y_test) to evaluate your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2018-07-31 16:53:37,184][cascade_classifier.transform] X_groups_test.shape=[(507, 21)]\n",
      "[ 2018-07-31 16:53:37,186][cascade_classifier.transform] group_dims=[21]\n",
      "[ 2018-07-31 16:53:37,186][cascade_classifier.transform] X_test.shape=(507, 21)\n",
      "[ 2018-07-31 16:53:37,188][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(507, 21)\n",
      "[ 2018-07-31 16:53:41,036][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(507, 27)\n",
      "[ 2018-07-31 16:53:43,704][cascade_classifier.transform] [layer=2] look_indexs=[0], X_cur_test.shape=(507, 27)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of GCForest = 97.830375 %\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "y_pred = gc.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy of GCForest = {:.6f} %\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' Time ', '101.735', ' seconds')\n",
      "[[493   1]\n",
      " [ 10   3]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.98      1.00      0.99       494\n",
      "        1.0       0.75      0.23      0.35        13\n",
      "\n",
      "avg / total       0.97      0.98      0.97       507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tt = time() - t0     \n",
    "print (\" Time \",format(round(tt,3)),\" seconds\")\n",
    "# Matrix de confusion\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of DecisionTreeClassifier = 97.633136 %\n",
      "(' Time ', '0.009', ' seconds')\n",
      "[[491   3]\n",
      " [  9   4]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.98      0.99      0.99       494\n",
      "        1.0       0.57      0.31      0.40        13\n",
      "\n",
      "avg / total       0.97      0.98      0.97       507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy of DecisionTreeClassifier = {:.6f} %\".format(acc * 100))\n",
    "\n",
    "tt = time() - t0     \n",
    "print (\" Time \",format(round(tt,3)),\" seconds\")\n",
    "# Matrix de confusion\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of RandomForestClassifier = 97.435897 %\n",
      "(' Time ', '0.402', ' seconds')\n",
      "[[493   1]\n",
      " [ 12   1]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.98      1.00      0.99       494\n",
      "        1.0       0.50      0.08      0.13        13\n",
      "\n",
      "avg / total       0.96      0.97      0.97       507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy of RandomForestClassifier = {:.6f} %\".format(acc * 100))\n",
    "\n",
    "tt = time() - t0     \n",
    "print (\" Time \",format(round(tt,3)),\" seconds\")\n",
    "# Matrix de confusion\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of ExtraTreesClassifier = 97.633136 %\n",
      "(' Time ', '0.35', ' seconds')\n",
      "[[494   0]\n",
      " [ 12   1]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.98      1.00      0.99       494\n",
      "        1.0       1.00      0.08      0.14        13\n",
      "\n",
      "avg / total       0.98      0.98      0.97       507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf = ExtraTreesClassifier(n_estimators=100)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy of ExtraTreesClassifier = {:.6f} %\".format(acc * 100))\n",
    "\n",
    "tt = time() - t0     \n",
    "print (\" Time \",format(round(tt,3)),\" seconds\")\n",
    "# Matrix de confusion\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of AdaBoostClassifier = 97.238659 %\n",
      "(' Time ', '0.322', ' seconds')\n",
      "[[491   3]\n",
      " [ 11   2]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.98      0.99      0.99       494\n",
      "        1.0       0.40      0.15      0.22        13\n",
      "\n",
      "avg / total       0.96      0.97      0.97       507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy of AdaBoostClassifier = {:.6f} %\".format(acc * 100))\n",
    "\n",
    "tt = time() - t0     \n",
    "print (\" Time \",format(round(tt,3)),\" seconds\")\n",
    "# Matrix de confusion\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Naive_bayes  GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of GaussianNB = 93.293886 %\n",
      "(' Time ', '0.003', ' seconds')\n",
      "[[467  27]\n",
      " [  7   6]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.99      0.95      0.96       494\n",
      "        1.0       0.18      0.46      0.26        13\n",
      "\n",
      "avg / total       0.96      0.93      0.95       507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb = gnb.fit(X_train, y_train)\n",
    "\n",
    "y_pred= gnb.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy of GaussianNB = {:.6f} %\".format(acc * 100))\n",
    "\n",
    "tt = time() - t0     \n",
    "print (\" Time \",format(round(tt,3)),\" seconds\")\n",
    "# Matrix de confusion\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1062/1062 [==============================] - 0s 461us/step - loss: 0.2524 - acc: 0.9831\n",
      "Epoch 2/50\n",
      "1062/1062 [==============================] - 0s 179us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 3/50\n",
      "1062/1062 [==============================] - 0s 187us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 4/50\n",
      "1062/1062 [==============================] - 0s 192us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 5/50\n",
      "1062/1062 [==============================] - 0s 192us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 6/50\n",
      "1062/1062 [==============================] - 0s 181us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 7/50\n",
      "1062/1062 [==============================] - 0s 182us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 8/50\n",
      "1062/1062 [==============================] - 0s 188us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 9/50\n",
      "1062/1062 [==============================] - 0s 185us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 10/50\n",
      "1062/1062 [==============================] - 0s 186us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 11/50\n",
      "1062/1062 [==============================] - 0s 187us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 12/50\n",
      "1062/1062 [==============================] - 0s 186us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 13/50\n",
      "1062/1062 [==============================] - 0s 174us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 14/50\n",
      "1062/1062 [==============================] - 0s 175us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 15/50\n",
      "1062/1062 [==============================] - 0s 182us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 16/50\n",
      "1062/1062 [==============================] - 0s 192us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 17/50\n",
      "1062/1062 [==============================] - 0s 188us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 18/50\n",
      "1062/1062 [==============================] - 0s 200us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 19/50\n",
      "1062/1062 [==============================] - 0s 212us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 20/50\n",
      "1062/1062 [==============================] - 0s 213us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 21/50\n",
      "1062/1062 [==============================] - 0s 216us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 22/50\n",
      "1062/1062 [==============================] - 0s 192us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 23/50\n",
      "1062/1062 [==============================] - 0s 191us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 24/50\n",
      "1062/1062 [==============================] - 0s 189us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 25/50\n",
      "1062/1062 [==============================] - 0s 188us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 26/50\n",
      "1062/1062 [==============================] - 0s 192us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 27/50\n",
      "1062/1062 [==============================] - 0s 201us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 28/50\n",
      "1062/1062 [==============================] - 0s 201us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 29/50\n",
      "1062/1062 [==============================] - 0s 196us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 30/50\n",
      "1062/1062 [==============================] - 0s 201us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 31/50\n",
      "1062/1062 [==============================] - 0s 206us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 32/50\n",
      "1062/1062 [==============================] - 0s 188us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 33/50\n",
      "1062/1062 [==============================] - 0s 191us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 34/50\n",
      "1062/1062 [==============================] - 0s 192us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 35/50\n",
      "1062/1062 [==============================] - 0s 185us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 36/50\n",
      "1062/1062 [==============================] - 0s 205us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 37/50\n",
      "1062/1062 [==============================] - 0s 197us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 38/50\n",
      "1062/1062 [==============================] - 0s 199us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 39/50\n",
      "1062/1062 [==============================] - 0s 199us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 40/50\n",
      "1062/1062 [==============================] - 0s 188us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 41/50\n",
      "1062/1062 [==============================] - 0s 195us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 42/50\n",
      "1062/1062 [==============================] - 0s 182us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 43/50\n",
      "1062/1062 [==============================] - 0s 184us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 44/50\n",
      "1062/1062 [==============================] - 0s 179us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 45/50\n",
      "1062/1062 [==============================] - 0s 178us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 46/50\n",
      "1062/1062 [==============================] - 0s 168us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 47/50\n",
      "1062/1062 [==============================] - 0s 175us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 48/50\n",
      "1062/1062 [==============================] - 0s 172us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 49/50\n",
      "1062/1062 [==============================] - 0s 172us/step - loss: 0.2732 - acc: 0.9831\n",
      "Epoch 50/50\n",
      "1062/1062 [==============================] - 0s 175us/step - loss: 0.2732 - acc: 0.9831\n",
      "119/119 [==============================] - 0s 173us/step\n",
      "Epoch 1/50\n",
      "1063/1063 [==============================] - 1s 589us/step - loss: 0.2833 - acc: 0.9831\n",
      "Epoch 2/50\n",
      "1063/1063 [==============================] - 0s 168us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 3/50\n",
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 4/50\n",
      "1063/1063 [==============================] - 0s 171us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 5/50\n",
      "1063/1063 [==============================] - 0s 168us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 6/50\n",
      "1063/1063 [==============================] - 0s 184us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 7/50\n",
      "1063/1063 [==============================] - 0s 254us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 8/50\n",
      "1063/1063 [==============================] - 0s 276us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 9/50\n",
      "1063/1063 [==============================] - 0s 262us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 10/50\n",
      "1063/1063 [==============================] - 0s 247us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 11/50\n",
      "1063/1063 [==============================] - 0s 205us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 12/50\n",
      "1063/1063 [==============================] - 0s 208us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 13/50\n",
      "1063/1063 [==============================] - 0s 258us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 14/50\n",
      "1063/1063 [==============================] - 0s 294us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 15/50\n",
      "1063/1063 [==============================] - 0s 275us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 16/50\n",
      "1063/1063 [==============================] - 0s 258us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 17/50\n",
      "1063/1063 [==============================] - 0s 260us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 18/50\n",
      "1063/1063 [==============================] - 0s 262us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 19/50\n",
      "1063/1063 [==============================] - 0s 278us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 20/50\n",
      "1063/1063 [==============================] - 0s 243us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 21/50\n",
      "1063/1063 [==============================] - 0s 286us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 22/50\n",
      "1063/1063 [==============================] - 0s 273us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 23/50\n",
      "1063/1063 [==============================] - 0s 251us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 24/50\n",
      "1063/1063 [==============================] - 0s 291us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 25/50\n",
      "1063/1063 [==============================] - 0s 253us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 26/50\n",
      "1063/1063 [==============================] - 0s 246us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 27/50\n",
      "1063/1063 [==============================] - 0s 215us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 28/50\n",
      "1063/1063 [==============================] - 0s 181us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 29/50\n",
      "1063/1063 [==============================] - 0s 191us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 30/50\n",
      "1063/1063 [==============================] - 0s 172us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 31/50\n",
      "1063/1063 [==============================] - 0s 174us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1063/1063 [==============================] - 0s 182us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 33/50\n",
      "1063/1063 [==============================] - 0s 171us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 34/50\n",
      "1063/1063 [==============================] - 0s 172us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 35/50\n",
      "1063/1063 [==============================] - 0s 165us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 36/50\n",
      "1063/1063 [==============================] - 0s 174us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 37/50\n",
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 38/50\n",
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 39/50\n",
      "1063/1063 [==============================] - 0s 170us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 40/50\n",
      "1063/1063 [==============================] - 0s 178us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 41/50\n",
      "1063/1063 [==============================] - 0s 178us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 42/50\n",
      "1063/1063 [==============================] - 0s 172us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 43/50\n",
      "1063/1063 [==============================] - 0s 176us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 44/50\n",
      "1063/1063 [==============================] - 0s 174us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 45/50\n",
      "1063/1063 [==============================] - 0s 176us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 46/50\n",
      "1063/1063 [==============================] - 0s 172us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 47/50\n",
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 48/50\n",
      "1063/1063 [==============================] - 0s 169us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 49/50\n",
      "1063/1063 [==============================] - 0s 169us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 50/50\n",
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.2729 - acc: 0.9831\n",
      "118/118 [==============================] - 0s 268us/step\n",
      "Epoch 1/50\n",
      "1063/1063 [==============================] - 1s 642us/step - loss: 0.3581 - acc: 0.9520\n",
      "Epoch 2/50\n",
      "1063/1063 [==============================] - 0s 176us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 3/50\n",
      "1063/1063 [==============================] - 0s 175us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 4/50\n",
      "1063/1063 [==============================] - 0s 176us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 5/50\n",
      "1063/1063 [==============================] - 0s 170us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 6/50\n",
      "1063/1063 [==============================] - 0s 171us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 7/50\n",
      "1063/1063 [==============================] - 0s 170us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 8/50\n",
      "1063/1063 [==============================] - 0s 170us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 9/50\n",
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 10/50\n",
      "1063/1063 [==============================] - 0s 176us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 11/50\n",
      "1063/1063 [==============================] - 0s 178us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 12/50\n",
      "1063/1063 [==============================] - 0s 180us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 13/50\n",
      "1063/1063 [==============================] - 0s 173us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 14/50\n",
      "1063/1063 [==============================] - 0s 172us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 15/50\n",
      "1063/1063 [==============================] - 0s 171us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 16/50\n",
      "1063/1063 [==============================] - 0s 174us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 17/50\n",
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 18/50\n",
      "1063/1063 [==============================] - 0s 175us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 19/50\n",
      "1063/1063 [==============================] - 0s 174us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 20/50\n",
      "1063/1063 [==============================] - 0s 172us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 21/50\n",
      "1063/1063 [==============================] - 0s 176us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 22/50\n",
      "1063/1063 [==============================] - 0s 170us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 23/50\n",
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 24/50\n",
      "1063/1063 [==============================] - ETA: 0s - loss: 0.3022 - acc: 0.981 - 0s 188us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 25/50\n",
      "1063/1063 [==============================] - 0s 176us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 26/50\n",
      "1063/1063 [==============================] - 0s 175us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 27/50\n",
      "1063/1063 [==============================] - 0s 170us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 28/50\n",
      "1063/1063 [==============================] - 0s 183us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 29/50\n",
      "1063/1063 [==============================] - 0s 175us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 30/50\n",
      "1063/1063 [==============================] - 0s 171us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 31/50\n",
      "1063/1063 [==============================] - 0s 173us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 32/50\n",
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 33/50\n",
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 34/50\n",
      "1063/1063 [==============================] - 0s 185us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 35/50\n",
      "1063/1063 [==============================] - 0s 179us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 36/50\n",
      "1063/1063 [==============================] - 0s 178us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 37/50\n",
      "1063/1063 [==============================] - 0s 175us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 38/50\n",
      "1063/1063 [==============================] - 0s 175us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 39/50\n",
      "1063/1063 [==============================] - 0s 181us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 40/50\n",
      "1063/1063 [==============================] - 0s 182us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 41/50\n",
      "1063/1063 [==============================] - 0s 185us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 42/50\n",
      "1063/1063 [==============================] - 0s 176us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 43/50\n",
      "1063/1063 [==============================] - 0s 182us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 44/50\n",
      "1063/1063 [==============================] - 0s 184us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 45/50\n",
      "1063/1063 [==============================] - 0s 178us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 46/50\n",
      "1063/1063 [==============================] - 0s 173us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 47/50\n",
      "1063/1063 [==============================] - 0s 173us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 48/50\n",
      "1063/1063 [==============================] - 0s 176us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 49/50\n",
      "1063/1063 [==============================] - 0s 171us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 50/50\n",
      "1063/1063 [==============================] - 0s 178us/step - loss: 0.2881 - acc: 0.9821\n",
      "118/118 [==============================] - 0s 362us/step\n",
      "Epoch 1/50\n",
      "1063/1063 [==============================] - 1s 527us/step - loss: 0.2730 - acc: 0.9831\n",
      "Epoch 2/50\n",
      "1063/1063 [==============================] - 0s 173us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 3/50\n",
      "1063/1063 [==============================] - 0s 169us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 4/50\n",
      "1063/1063 [==============================] - 0s 173us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 5/50\n",
      "1063/1063 [==============================] - 0s 176us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 6/50\n",
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 7/50\n",
      "1063/1063 [==============================] - 0s 169us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 8/50\n",
      "1063/1063 [==============================] - 0s 178us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 9/50\n",
      "1063/1063 [==============================] - 0s 178us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 10/50\n",
      "1063/1063 [==============================] - 0s 176us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 11/50\n",
      "1063/1063 [==============================] - 0s 178us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1063/1063 [==============================] - 0s 174us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 13/50\n",
      "1063/1063 [==============================] - 0s 170us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 14/50\n",
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 15/50\n",
      "1063/1063 [==============================] - 0s 172us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 16/50\n",
      "1063/1063 [==============================] - 0s 181us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 17/50\n",
      "1063/1063 [==============================] - 0s 169us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 18/50\n",
      "1063/1063 [==============================] - 0s 175us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 19/50\n",
      "1063/1063 [==============================] - 0s 173us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 20/50\n",
      "1063/1063 [==============================] - 0s 174us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 21/50\n",
      "1063/1063 [==============================] - 0s 173us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 22/50\n",
      "1063/1063 [==============================] - 0s 171us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 23/50\n",
      "1063/1063 [==============================] - 0s 170us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 24/50\n",
      "1063/1063 [==============================] - 0s 172us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 25/50\n",
      "1063/1063 [==============================] - 0s 173us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 26/50\n",
      "1063/1063 [==============================] - 0s 178us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 27/50\n",
      "1063/1063 [==============================] - 0s 179us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 28/50\n",
      "1063/1063 [==============================] - 0s 184us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 29/50\n",
      "1063/1063 [==============================] - 0s 178us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 30/50\n",
      "1063/1063 [==============================] - 0s 178us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 31/50\n",
      "1063/1063 [==============================] - 0s 172us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 32/50\n",
      "1063/1063 [==============================] - 0s 171us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 33/50\n",
      "1063/1063 [==============================] - 0s 178us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 34/50\n",
      "1063/1063 [==============================] - 0s 172us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 35/50\n",
      "1063/1063 [==============================] - 0s 171us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 36/50\n",
      "1063/1063 [==============================] - 0s 169us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 37/50\n",
      "1063/1063 [==============================] - 0s 172us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 38/50\n",
      "1063/1063 [==============================] - 0s 171us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 39/50\n",
      "1063/1063 [==============================] - 0s 176us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 40/50\n",
      "1063/1063 [==============================] - 0s 180us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 41/50\n",
      "1063/1063 [==============================] - 0s 175us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 42/50\n",
      "1063/1063 [==============================] - 0s 175us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 43/50\n",
      "1063/1063 [==============================] - 0s 172us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 44/50\n",
      "1063/1063 [==============================] - 0s 175us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 45/50\n",
      "1063/1063 [==============================] - 0s 170us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 46/50\n",
      "1063/1063 [==============================] - 0s 174us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 47/50\n",
      "1063/1063 [==============================] - 0s 170us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 48/50\n",
      "1063/1063 [==============================] - 0s 169us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 49/50\n",
      "1063/1063 [==============================] - 0s 171us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 50/50\n",
      "1063/1063 [==============================] - 0s 180us/step - loss: 0.2729 - acc: 0.9831\n",
      "118/118 [==============================] - 0s 425us/step\n",
      "Epoch 1/50\n",
      "1063/1063 [==============================] - 1s 534us/step - loss: 0.5527 - acc: 0.9548\n",
      "Epoch 2/50\n",
      "1063/1063 [==============================] - 0s 170us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 3/50\n",
      "1063/1063 [==============================] - 0s 166us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 4/50\n",
      "1063/1063 [==============================] - 0s 165us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 5/50\n",
      "1063/1063 [==============================] - 0s 173us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 6/50\n",
      "1063/1063 [==============================] - 0s 174us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 7/50\n",
      "1063/1063 [==============================] - 0s 171us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 8/50\n",
      "1063/1063 [==============================] - 0s 171us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 9/50\n",
      "1063/1063 [==============================] - 0s 176us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 10/50\n",
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 11/50\n",
      "1063/1063 [==============================] - 0s 174us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 12/50\n",
      "1063/1063 [==============================] - 0s 171us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 13/50\n",
      "1063/1063 [==============================] - 0s 178us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 14/50\n",
      "1063/1063 [==============================] - 0s 174us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 15/50\n",
      "1063/1063 [==============================] - 0s 179us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 16/50\n",
      "1063/1063 [==============================] - 0s 179us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 17/50\n",
      "1063/1063 [==============================] - 0s 176us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 18/50\n",
      "1063/1063 [==============================] - 0s 181us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 19/50\n",
      "1063/1063 [==============================] - 0s 175us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 20/50\n",
      "1063/1063 [==============================] - 0s 192us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 21/50\n",
      "1063/1063 [==============================] - 0s 179us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 22/50\n",
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 23/50\n",
      "1063/1063 [==============================] - 0s 178us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 24/50\n",
      "1063/1063 [==============================] - 0s 172us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 25/50\n",
      "1063/1063 [==============================] - 0s 175us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 26/50\n",
      "1063/1063 [==============================] - 0s 176us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 27/50\n",
      "1063/1063 [==============================] - 0s 168us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 28/50\n",
      "1063/1063 [==============================] - 0s 167us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 29/50\n",
      "1063/1063 [==============================] - ETA: 0s - loss: 0.2518 - acc: 0.9844  - 0s 186us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 30/50\n",
      "1063/1063 [==============================] - 0s 176us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 31/50\n",
      "1063/1063 [==============================] - 0s 179us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 32/50\n",
      "1063/1063 [==============================] - 0s 170us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 33/50\n",
      "1063/1063 [==============================] - 0s 172us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 34/50\n",
      "1063/1063 [==============================] - 0s 173us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 35/50\n",
      "1063/1063 [==============================] - 0s 171us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 36/50\n",
      "1063/1063 [==============================] - 0s 172us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 37/50\n",
      "1063/1063 [==============================] - 0s 178us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 38/50\n",
      "1063/1063 [==============================] - 0s 178us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 39/50\n",
      "1063/1063 [==============================] - 0s 173us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 40/50\n",
      "1063/1063 [==============================] - 0s 171us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 41/50\n",
      "1063/1063 [==============================] - 0s 180us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 42/50\n",
      "1063/1063 [==============================] - 0s 174us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 44/50\n",
      "1063/1063 [==============================] - 0s 174us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 45/50\n",
      "1063/1063 [==============================] - 0s 170us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 46/50\n",
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 47/50\n",
      "1063/1063 [==============================] - 0s 174us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 48/50\n",
      "1063/1063 [==============================] - 0s 176us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 49/50\n",
      "1063/1063 [==============================] - 0s 172us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 50/50\n",
      "1063/1063 [==============================] - 0s 175us/step - loss: 0.2729 - acc: 0.9831\n",
      "118/118 [==============================] - 0s 543us/step\n",
      "Epoch 1/50\n",
      "1063/1063 [==============================] - 1s 761us/step - loss: 0.2743 - acc: 0.9831\n",
      "Epoch 2/50\n",
      "1063/1063 [==============================] - 0s 175us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 3/50\n",
      "1063/1063 [==============================] - 0s 172us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 4/50\n",
      "1063/1063 [==============================] - 0s 176us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 5/50\n",
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 6/50\n",
      "1063/1063 [==============================] - 0s 179us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 7/50\n",
      "1063/1063 [==============================] - 0s 176us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 8/50\n",
      "1063/1063 [==============================] - 0s 171us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 9/50\n",
      "1063/1063 [==============================] - 0s 176us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 10/50\n",
      "1063/1063 [==============================] - 0s 170us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 11/50\n",
      "1063/1063 [==============================] - 0s 176us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 12/50\n",
      "1063/1063 [==============================] - 0s 174us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 13/50\n",
      "1063/1063 [==============================] - 0s 172us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 14/50\n",
      "1063/1063 [==============================] - 0s 168us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 15/50\n",
      "1063/1063 [==============================] - 0s 176us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 16/50\n",
      "1063/1063 [==============================] - 0s 193us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 17/50\n",
      "1063/1063 [==============================] - 0s 176us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 18/50\n",
      "1063/1063 [==============================] - 0s 178us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 19/50\n",
      "1063/1063 [==============================] - 0s 176us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 20/50\n",
      "1063/1063 [==============================] - 0s 166us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 21/50\n",
      "1063/1063 [==============================] - 0s 171us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 22/50\n",
      "1063/1063 [==============================] - 0s 171us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 23/50\n",
      "1063/1063 [==============================] - 0s 169us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 24/50\n",
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 25/50\n",
      "1063/1063 [==============================] - 0s 171us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 26/50\n",
      "1063/1063 [==============================] - 0s 190us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 27/50\n",
      "1063/1063 [==============================] - 0s 166us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 28/50\n",
      "1063/1063 [==============================] - 0s 173us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 29/50\n",
      "1063/1063 [==============================] - 0s 173us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 30/50\n",
      "1063/1063 [==============================] - 0s 168us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 31/50\n",
      "1063/1063 [==============================] - 0s 185us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 32/50\n",
      "1063/1063 [==============================] - 0s 189us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 33/50\n",
      "1063/1063 [==============================] - 0s 185us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 34/50\n",
      "1063/1063 [==============================] - 0s 190us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 35/50\n",
      "1063/1063 [==============================] - 0s 189us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 36/50\n",
      "1063/1063 [==============================] - 0s 185us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 37/50\n",
      "1063/1063 [==============================] - 0s 180us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 38/50\n",
      "1063/1063 [==============================] - 0s 190us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 39/50\n",
      "1063/1063 [==============================] - 0s 197us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 40/50\n",
      "1063/1063 [==============================] - 0s 196us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 41/50\n",
      "1063/1063 [==============================] - 0s 203us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 42/50\n",
      "1063/1063 [==============================] - 0s 203us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 43/50\n",
      "1063/1063 [==============================] - 0s 198us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 44/50\n",
      "1063/1063 [==============================] - 0s 210us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 45/50\n",
      "1063/1063 [==============================] - 0s 200us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 46/50\n",
      "1063/1063 [==============================] - 0s 198us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 47/50\n",
      "1063/1063 [==============================] - 0s 204us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 48/50\n",
      "1063/1063 [==============================] - 0s 184us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 49/50\n",
      "1063/1063 [==============================] - 0s 181us/step - loss: 0.2729 - acc: 0.9831\n",
      "Epoch 50/50\n",
      "1063/1063 [==============================] - 0s 186us/step - loss: 0.2729 - acc: 0.9831\n",
      "118/118 [==============================] - 0s 655us/step\n",
      "Epoch 1/50\n",
      "1063/1063 [==============================] - 1s 654us/step - loss: 0.5992 - acc: 0.9539\n",
      "Epoch 2/50\n",
      "1063/1063 [==============================] - 0s 199us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 3/50\n",
      "1063/1063 [==============================] - 0s 186us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 4/50\n",
      "1063/1063 [==============================] - 0s 199us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 5/50\n",
      "1063/1063 [==============================] - 0s 196us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 6/50\n",
      "1063/1063 [==============================] - 0s 194us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 7/50\n",
      "1063/1063 [==============================] - 0s 195us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 8/50\n",
      "1063/1063 [==============================] - 0s 210us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 9/50\n",
      "1063/1063 [==============================] - 0s 212us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 10/50\n",
      "1063/1063 [==============================] - 0s 202us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 11/50\n",
      "1063/1063 [==============================] - 0s 193us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 12/50\n",
      "1063/1063 [==============================] - 0s 203us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 13/50\n",
      "1063/1063 [==============================] - 0s 191us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 14/50\n",
      "1063/1063 [==============================] - 0s 189us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 15/50\n",
      "1063/1063 [==============================] - 0s 184us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 16/50\n",
      "1063/1063 [==============================] - 0s 185us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 17/50\n",
      "1063/1063 [==============================] - 0s 197us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 18/50\n",
      "1063/1063 [==============================] - 0s 193us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 19/50\n",
      "1063/1063 [==============================] - 0s 182us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 20/50\n",
      "1063/1063 [==============================] - 0s 174us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 21/50\n",
      "1063/1063 [==============================] - 0s 170us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 22/50\n",
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 23/50\n",
      "1063/1063 [==============================] - 0s 181us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1063/1063 [==============================] - 0s 180us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 25/50\n",
      "1063/1063 [==============================] - 0s 175us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 26/50\n",
      "1063/1063 [==============================] - 0s 176us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 27/50\n",
      "1063/1063 [==============================] - 0s 186us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 28/50\n",
      "1063/1063 [==============================] - 0s 200us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 29/50\n",
      "1063/1063 [==============================] - 0s 192us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 30/50\n",
      "1063/1063 [==============================] - 0s 179us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 31/50\n",
      "1063/1063 [==============================] - 0s 199us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 32/50\n",
      "1063/1063 [==============================] - 0s 243us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 33/50\n",
      "1063/1063 [==============================] - 0s 223us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 34/50\n",
      "1063/1063 [==============================] - 0s 187us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 35/50\n",
      "1063/1063 [==============================] - 0s 178us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 36/50\n",
      "1063/1063 [==============================] - 0s 181us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 37/50\n",
      "1063/1063 [==============================] - 0s 172us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 38/50\n",
      "1063/1063 [==============================] - 0s 175us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 39/50\n",
      "1063/1063 [==============================] - 0s 183us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 40/50\n",
      "1063/1063 [==============================] - 0s 178us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 41/50\n",
      "1063/1063 [==============================] - 0s 184us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 42/50\n",
      "1063/1063 [==============================] - 0s 179us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 43/50\n",
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 44/50\n",
      "1063/1063 [==============================] - 0s 180us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 45/50\n",
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 46/50\n",
      "1063/1063 [==============================] - 0s 186us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 47/50\n",
      "1063/1063 [==============================] - 0s 181us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 48/50\n",
      "1063/1063 [==============================] - 0s 181us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 49/50\n",
      "1063/1063 [==============================] - 0s 174us/step - loss: 0.2578 - acc: 0.9840\n",
      "Epoch 50/50\n",
      "1063/1063 [==============================] - 0s 186us/step - loss: 0.2578 - acc: 0.9840\n",
      "118/118 [==============================] - 0s 829us/step\n",
      "Epoch 1/50\n",
      "1063/1063 [==============================] - 1s 836us/step - loss: 0.3581 - acc: 0.9548\n",
      "Epoch 2/50\n",
      "1063/1063 [==============================] - 0s 181us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 3/50\n",
      "1063/1063 [==============================] - 0s 192us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 4/50\n",
      "1063/1063 [==============================] - 0s 238us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 5/50\n",
      "1063/1063 [==============================] - 0s 219us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 6/50\n",
      "1063/1063 [==============================] - 0s 181us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 7/50\n",
      "1063/1063 [==============================] - 0s 185us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 8/50\n",
      "1063/1063 [==============================] - 0s 194us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 9/50\n",
      "1063/1063 [==============================] - ETA: 0s - loss: 0.2432 - acc: 0.9849  - 0s 186us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 10/50\n",
      "1063/1063 [==============================] - 0s 181us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 11/50\n",
      "1063/1063 [==============================] - 0s 181us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 12/50\n",
      "1063/1063 [==============================] - 0s 181us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 13/50\n",
      "1063/1063 [==============================] - 0s 190us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 14/50\n",
      "1063/1063 [==============================] - 0s 183us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 15/50\n",
      "1063/1063 [==============================] - 0s 179us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 16/50\n",
      "1063/1063 [==============================] - 0s 190us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 17/50\n",
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 18/50\n",
      "1063/1063 [==============================] - 0s 185us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 19/50\n",
      "1063/1063 [==============================] - 0s 178us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 20/50\n",
      "1063/1063 [==============================] - 0s 172us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 21/50\n",
      "1063/1063 [==============================] - 0s 176us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 22/50\n",
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 23/50\n",
      "1063/1063 [==============================] - 0s 179us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 24/50\n",
      "1063/1063 [==============================] - 0s 178us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 25/50\n",
      "1063/1063 [==============================] - 0s 174us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 26/50\n",
      "1063/1063 [==============================] - 0s 178us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 27/50\n",
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 28/50\n",
      "1063/1063 [==============================] - 0s 172us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 29/50\n",
      "1063/1063 [==============================] - 0s 178us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 30/50\n",
      "1063/1063 [==============================] - 0s 176us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 31/50\n",
      "1063/1063 [==============================] - 0s 175us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 32/50\n",
      "1063/1063 [==============================] - 0s 203us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 33/50\n",
      "1063/1063 [==============================] - 0s 185us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 34/50\n",
      "1063/1063 [==============================] - 0s 182us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 35/50\n",
      "1063/1063 [==============================] - 0s 176us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 36/50\n",
      "1063/1063 [==============================] - 0s 171us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 37/50\n",
      "1063/1063 [==============================] - 0s 174us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 38/50\n",
      "1063/1063 [==============================] - 0s 180us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 39/50\n",
      "1063/1063 [==============================] - 0s 179us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 40/50\n",
      "1063/1063 [==============================] - 0s 184us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 41/50\n",
      "1063/1063 [==============================] - 0s 183us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 42/50\n",
      "1063/1063 [==============================] - 0s 183us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 43/50\n",
      "1063/1063 [==============================] - 0s 189us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 44/50\n",
      "1063/1063 [==============================] - 0s 186us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 45/50\n",
      "1063/1063 [==============================] - 0s 181us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 46/50\n",
      "1063/1063 [==============================] - 0s 189us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 47/50\n",
      "1063/1063 [==============================] - 0s 193us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 48/50\n",
      "1063/1063 [==============================] - 0s 191us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 49/50\n",
      "1063/1063 [==============================] - 0s 207us/step - loss: 0.3033 - acc: 0.9812\n",
      "Epoch 50/50\n",
      "1063/1063 [==============================] - 0s 209us/step - loss: 0.3033 - acc: 0.9812\n",
      "118/118 [==============================] - 0s 973us/step\n",
      "Epoch 1/50\n",
      "1063/1063 [==============================] - 1s 983us/step - loss: 0.2470 - acc: 0.9746\n",
      "Epoch 2/50\n",
      "1063/1063 [==============================] - 0s 283us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 3/50\n",
      "1063/1063 [==============================] - 0s 312us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1063/1063 [==============================] - 0s 296us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 5/50\n",
      "1063/1063 [==============================] - 0s 328us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 6/50\n",
      "1063/1063 [==============================] - 0s 308us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 7/50\n",
      "1063/1063 [==============================] - 0s 361us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 8/50\n",
      "1063/1063 [==============================] - 0s 363us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 9/50\n",
      "1063/1063 [==============================] - 0s 257us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 10/50\n",
      "1063/1063 [==============================] - 0s 288us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 11/50\n",
      "1063/1063 [==============================] - 0s 275us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 12/50\n",
      "1063/1063 [==============================] - 0s 328us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 13/50\n",
      "1063/1063 [==============================] - 0s 319us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 14/50\n",
      "1063/1063 [==============================] - 0s 273us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 15/50\n",
      "1063/1063 [==============================] - 0s 253us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 16/50\n",
      "1063/1063 [==============================] - 0s 233us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 17/50\n",
      "1063/1063 [==============================] - 0s 284us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 18/50\n",
      "1063/1063 [==============================] - 0s 352us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 19/50\n",
      "1063/1063 [==============================] - 0s 323us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 20/50\n",
      "1063/1063 [==============================] - 0s 327us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 21/50\n",
      "1063/1063 [==============================] - 0s 263us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 22/50\n",
      "1063/1063 [==============================] - 0s 246us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 23/50\n",
      "1063/1063 [==============================] - 0s 316us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 24/50\n",
      "1063/1063 [==============================] - 0s 256us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 25/50\n",
      "1063/1063 [==============================] - 0s 203us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 26/50\n",
      "1063/1063 [==============================] - 0s 181us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 27/50\n",
      "1063/1063 [==============================] - 0s 185us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 28/50\n",
      "1063/1063 [==============================] - 0s 184us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 29/50\n",
      "1063/1063 [==============================] - 0s 193us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 30/50\n",
      "1063/1063 [==============================] - 0s 183us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 31/50\n",
      "1063/1063 [==============================] - 0s 176us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 32/50\n",
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 33/50\n",
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 34/50\n",
      "1063/1063 [==============================] - 0s 175us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 35/50\n",
      "1063/1063 [==============================] - 0s 173us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 36/50\n",
      "1063/1063 [==============================] - 0s 186us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 37/50\n",
      "1063/1063 [==============================] - 0s 185us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 38/50\n",
      "1063/1063 [==============================] - 0s 181us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 39/50\n",
      "1063/1063 [==============================] - 0s 176us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 40/50\n",
      "1063/1063 [==============================] - 0s 180us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 41/50\n",
      "1063/1063 [==============================] - 0s 180us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 42/50\n",
      "1063/1063 [==============================] - 0s 181us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 43/50\n",
      "1063/1063 [==============================] - 0s 178us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 44/50\n",
      "1063/1063 [==============================] - 0s 172us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 45/50\n",
      "1063/1063 [==============================] - 0s 181us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 46/50\n",
      "1063/1063 [==============================] - 0s 181us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 47/50\n",
      "1063/1063 [==============================] - 0s 182us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 48/50\n",
      "1063/1063 [==============================] - 0s 188us/step - loss: 0.2274 - acc: 0.9859TA: 0s - loss: 0.2339 - acc: 0.985\n",
      "Epoch 49/50\n",
      "1063/1063 [==============================] - 0s 185us/step - loss: 0.2274 - acc: 0.9859\n",
      "Epoch 50/50\n",
      "1063/1063 [==============================] - 0s 186us/step - loss: 0.2274 - acc: 0.9859\n",
      "118/118 [==============================] - 0s 1ms/step\n",
      "Epoch 1/50\n",
      "1063/1063 [==============================] - 1s 691us/step - loss: 0.3217 - acc: 0.9558\n",
      "Epoch 2/50\n",
      "1063/1063 [==============================] - 0s 181us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 3/50\n",
      "1063/1063 [==============================] - 0s 182us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 4/50\n",
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 5/50\n",
      "1063/1063 [==============================] - 0s 179us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 6/50\n",
      "1063/1063 [==============================] - 0s 187us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 7/50\n",
      "1063/1063 [==============================] - 0s 176us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 8/50\n",
      "1063/1063 [==============================] - 0s 183us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 9/50\n",
      "1063/1063 [==============================] - 0s 180us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 10/50\n",
      "1063/1063 [==============================] - 0s 180us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 11/50\n",
      "1063/1063 [==============================] - 0s 184us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 12/50\n",
      "1063/1063 [==============================] - 0s 174us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 13/50\n",
      "1063/1063 [==============================] - 0s 178us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 14/50\n",
      "1063/1063 [==============================] - 0s 174us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 15/50\n",
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 16/50\n",
      "1063/1063 [==============================] - 0s 176us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 17/50\n",
      "1063/1063 [==============================] - 0s 201us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 18/50\n",
      "1063/1063 [==============================] - 0s 179us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 19/50\n",
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 20/50\n",
      "1063/1063 [==============================] - 0s 174us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 21/50\n",
      "1063/1063 [==============================] - 0s 182us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 22/50\n",
      "1063/1063 [==============================] - 0s 185us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 23/50\n",
      "1063/1063 [==============================] - 0s 181us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 24/50\n",
      "1063/1063 [==============================] - 0s 182us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 25/50\n",
      "1063/1063 [==============================] - 0s 181us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 26/50\n",
      "1063/1063 [==============================] - 0s 180us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 27/50\n",
      "1063/1063 [==============================] - 0s 184us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 28/50\n",
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 29/50\n",
      "1063/1063 [==============================] - 0s 173us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 30/50\n",
      "1063/1063 [==============================] - 0s 179us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 31/50\n",
      "1063/1063 [==============================] - 0s 181us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 32/50\n",
      "1063/1063 [==============================] - 0s 181us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 33/50\n",
      "1063/1063 [==============================] - 0s 178us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 34/50\n",
      "1063/1063 [==============================] - 0s 191us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1063/1063 [==============================] - 0s 191us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 36/50\n",
      "1063/1063 [==============================] - 0s 186us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 37/50\n",
      "1063/1063 [==============================] - 0s 174us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 38/50\n",
      "1063/1063 [==============================] - 0s 181us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 39/50\n",
      "1063/1063 [==============================] - 0s 181us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 40/50\n",
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 41/50\n",
      "1063/1063 [==============================] - 0s 181us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 42/50\n",
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 43/50\n",
      "1063/1063 [==============================] - 0s 178us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 44/50\n",
      "1063/1063 [==============================] - 0s 177us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 45/50\n",
      "1063/1063 [==============================] - 0s 204us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 46/50\n",
      "1063/1063 [==============================] - 0s 216us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 47/50\n",
      "1063/1063 [==============================] - 0s 203us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 48/50\n",
      "1063/1063 [==============================] - 0s 226us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 49/50\n",
      "1063/1063 [==============================] - 0s 217us/step - loss: 0.2881 - acc: 0.9821\n",
      "Epoch 50/50\n",
      "1063/1063 [==============================] - 0s 229us/step - loss: 0.2881 - acc: 0.9821\n",
      "118/118 [==============================] - 0s 1ms/step\n",
      "Accuracy mean: 0.983065089837\n",
      "Accuracy variance: 0.0107196711682\n",
      "(' Time ', '115.125', ' seconds')\n",
      "[[467  27]\n",
      " [  7   6]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.99      0.95      0.96       494\n",
      "        1.0       0.18      0.46      0.26        13\n",
      "\n",
      "avg / total       0.96      0.93      0.95       507\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# estimators \n",
    "# Evaluating the ANN\n",
    "t0 = time()\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential # initialize neural network library\n",
    "from keras.layers import Dense # build our layers library\n",
    "def build_classifier():\n",
    "    classifier = Sequential() # initialize neural network\n",
    "    classifier.add(Dense(units = 1024, kernel_initializer = 'uniform', activation = 'relu', input_dim = X_train.shape[1]))\n",
    "    classifier.add(Dense(units = 512, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier\n",
    "classifier = KerasClassifier(build_fn = build_classifier, epochs = 50)\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "mean = accuracies.mean()\n",
    "variance = accuracies.std()\n",
    "print(\"Accuracy mean: \"+ str(mean))\n",
    "print(\"Accuracy variance: \"+ str(variance))\n",
    "\n",
    "tt = time() - t0     \n",
    "print (\" Time \",format(round(tt,3)),\" seconds\")\n",
    "# Matrix de confusion\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
