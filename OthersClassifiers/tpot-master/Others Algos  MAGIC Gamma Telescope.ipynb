{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt \n",
    "from pprint import pprint\n",
    "from numpy import array\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Flength</th>\n",
       "      <th>Fwidth</th>\n",
       "      <th>Fsize</th>\n",
       "      <th>Fconc</th>\n",
       "      <th>Fconc1</th>\n",
       "      <th>Fasym</th>\n",
       "      <th>Fm3long</th>\n",
       "      <th>Fm3trans</th>\n",
       "      <th>Falpha</th>\n",
       "      <th>Fdist</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.7967</td>\n",
       "      <td>16.0021</td>\n",
       "      <td>2.6449</td>\n",
       "      <td>0.3918</td>\n",
       "      <td>0.1982</td>\n",
       "      <td>27.7004</td>\n",
       "      <td>22.0110</td>\n",
       "      <td>-8.2027</td>\n",
       "      <td>40.0920</td>\n",
       "      <td>81.8828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31.6036</td>\n",
       "      <td>11.7235</td>\n",
       "      <td>2.5185</td>\n",
       "      <td>0.5303</td>\n",
       "      <td>0.3773</td>\n",
       "      <td>26.2722</td>\n",
       "      <td>23.8238</td>\n",
       "      <td>-9.9574</td>\n",
       "      <td>6.3609</td>\n",
       "      <td>205.2610</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>162.0520</td>\n",
       "      <td>136.0310</td>\n",
       "      <td>4.0612</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>116.7410</td>\n",
       "      <td>-64.8580</td>\n",
       "      <td>-45.2160</td>\n",
       "      <td>76.9600</td>\n",
       "      <td>256.7880</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.8172</td>\n",
       "      <td>9.5728</td>\n",
       "      <td>2.3385</td>\n",
       "      <td>0.6147</td>\n",
       "      <td>0.3922</td>\n",
       "      <td>27.2107</td>\n",
       "      <td>-6.4633</td>\n",
       "      <td>-7.1513</td>\n",
       "      <td>10.4490</td>\n",
       "      <td>116.7370</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.1362</td>\n",
       "      <td>30.9205</td>\n",
       "      <td>3.1611</td>\n",
       "      <td>0.3168</td>\n",
       "      <td>0.1832</td>\n",
       "      <td>-5.5277</td>\n",
       "      <td>28.5525</td>\n",
       "      <td>21.8393</td>\n",
       "      <td>4.6480</td>\n",
       "      <td>356.4620</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Flength    Fwidth   Fsize   Fconc  Fconc1     Fasym  Fm3long  Fm3trans  \\\n",
       "0   28.7967   16.0021  2.6449  0.3918  0.1982   27.7004  22.0110   -8.2027   \n",
       "1   31.6036   11.7235  2.5185  0.5303  0.3773   26.2722  23.8238   -9.9574   \n",
       "2  162.0520  136.0310  4.0612  0.0374  0.0187  116.7410 -64.8580  -45.2160   \n",
       "3   23.8172    9.5728  2.3385  0.6147  0.3922   27.2107  -6.4633   -7.1513   \n",
       "4   75.1362   30.9205  3.1611  0.3168  0.1832   -5.5277  28.5525   21.8393   \n",
       "\n",
       "    Falpha     Fdist  Class  \n",
       "0  40.0920   81.8828      1  \n",
       "1   6.3609  205.2610      1  \n",
       "2  76.9600  256.7880      1  \n",
       "3  10.4490  116.7370      1  \n",
       "4   4.6480  356.4620      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "df=pd.read_csv('MAGIC Gamma Telescope Data.csv')  \n",
    "\n",
    "df['Class'] = df.Class.apply(lambda label: 1 if label == \"g\" else 0)\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df to values\n",
    "df = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GC Forest\n",
    "import argparse\n",
    "import sys\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score\n",
    "sys.path.insert(0, \"lib\")\n",
    "from gcforest.gcforest import GCForest\n",
    "from gcforest.gcforest import GCForest\n",
    "from gcforest.utils.config_utils import load_json\n",
    "config = load_json(\"./examples/aloi.json\")   \n",
    "gc = GCForest(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kader/anaconda3/envs/py27/lib/python2.7/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# train test \n",
    "from sklearn.cross_validation import train_test_split\n",
    "y = df[:,10]\n",
    "X = df[:,0:10]\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of class\n",
    "len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2019-01-12 18:20:57,057][cascade_classifier.fit_transform] X_groups_train.shape=[(13314, 10)],y_train.shape=(13314,),X_groups_test.shape=[(5706, 10)],y_test.shape=(5706,)\n",
      "[ 2019-01-12 18:20:57,059][cascade_classifier.fit_transform] group_dims=[10]\n",
      "[ 2019-01-12 18:20:57,061][cascade_classifier.fit_transform] group_starts=[0]\n",
      "[ 2019-01-12 18:20:57,063][cascade_classifier.fit_transform] group_ends=[10]\n",
      "[ 2019-01-12 18:20:57,064][cascade_classifier.fit_transform] X_train.shape=(13314, 10),X_test.shape=(5706, 10)\n",
      "[ 2019-01-12 18:20:57,068][cascade_classifier.fit_transform] [layer=0] look_indexs=[0], X_cur_train.shape=(13314, 10), X_cur_test.shape=(5706, 10)\n",
      "[ 2019-01-12 18:20:59,650][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_0.predict)=88.97%\n",
      "[ 2019-01-12 18:21:02,225][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_1.predict)=87.09%\n",
      "[ 2019-01-12 18:21:04,966][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_2.predict)=87.69%\n",
      "[ 2019-01-12 18:21:07,303][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_3.predict)=87.23%\n",
      "[ 2019-01-12 18:21:09,535][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_4.predict)=87.83%\n",
      "[ 2019-01-12 18:21:12,205][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_5.predict)=87.45%\n",
      "[ 2019-01-12 18:21:14,475][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_6.predict)=88.96%\n",
      "[ 2019-01-12 18:21:16,736][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_7.predict)=87.45%\n",
      "[ 2019-01-12 18:21:18,981][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_8.predict)=85.50%\n",
      "[ 2019-01-12 18:21:21,244][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_9.predict)=88.13%\n",
      "[ 2019-01-12 18:21:21,376][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_cv.predict)=87.63%\n",
      "[ 2019-01-12 18:21:21,380][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.test.predict)=88.26%\n",
      "[ 2019-01-12 18:21:22,290][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_0.predict)=86.80%\n",
      "[ 2019-01-12 18:21:23,320][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_1.predict)=88.66%\n",
      "[ 2019-01-12 18:21:24,323][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_2.predict)=87.01%\n",
      "[ 2019-01-12 18:21:25,321][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_3.predict)=87.08%\n",
      "[ 2019-01-12 18:21:26,312][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_4.predict)=87.75%\n",
      "[ 2019-01-12 18:21:27,300][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_5.predict)=86.25%\n",
      "[ 2019-01-12 18:21:28,305][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_6.predict)=87.15%\n",
      "[ 2019-01-12 18:21:29,316][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_7.predict)=86.78%\n",
      "[ 2019-01-12 18:21:30,313][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_8.predict)=86.93%\n",
      "[ 2019-01-12 18:21:31,356][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_9.predict)=87.30%\n",
      "[ 2019-01-12 18:21:31,471][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_cv.predict)=87.17%\n",
      "[ 2019-01-12 18:21:31,475][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.test.predict)=87.75%\n",
      "[ 2019-01-12 18:21:32,642][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_0.predict)=78.47%\n",
      "[ 2019-01-12 18:21:32,806][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_1.predict)=77.93%\n",
      "[ 2019-01-12 18:21:32,945][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_2.predict)=79.43%\n",
      "[ 2019-01-12 18:21:33,114][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_3.predict)=78.06%\n",
      "[ 2019-01-12 18:21:33,289][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_4.predict)=79.34%\n",
      "[ 2019-01-12 18:21:33,411][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_5.predict)=79.26%\n",
      "[ 2019-01-12 18:21:33,615][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_6.predict)=78.21%\n",
      "[ 2019-01-12 18:21:33,792][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_7.predict)=78.89%\n",
      "[ 2019-01-12 18:21:33,965][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_8.predict)=78.36%\n",
      "[ 2019-01-12 18:21:34,140][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_9.predict)=80.24%\n",
      "[ 2019-01-12 18:21:34,143][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_cv.predict)=78.82%\n",
      "[ 2019-01-12 18:21:34,144][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.test.predict)=79.09%\n",
      "[ 2019-01-12 18:21:34,145][cascade_classifier.calc_accuracy] Accuracy(layer_0 - train.classifier_average)=86.63%\n",
      "[ 2019-01-12 18:21:34,148][cascade_classifier.calc_accuracy] Accuracy(layer_0 - test.classifier_average)=87.12%\n",
      "[ 2019-01-12 18:21:34,150][cascade_classifier.fit_transform] [layer=1] look_indexs=[0], X_cur_train.shape=(13314, 16), X_cur_test.shape=(5706, 16)\n",
      "[ 2019-01-12 18:21:36,063][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_0.predict)=88.37%\n",
      "[ 2019-01-12 18:21:38,272][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_1.predict)=87.31%\n",
      "[ 2019-01-12 18:21:40,625][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_2.predict)=86.41%\n",
      "[ 2019-01-12 18:21:43,011][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_3.predict)=87.75%\n",
      "[ 2019-01-12 18:21:45,373][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_4.predict)=87.75%\n",
      "[ 2019-01-12 18:21:47,647][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_5.predict)=86.85%\n",
      "[ 2019-01-12 18:21:49,921][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_6.predict)=89.86%\n",
      "[ 2019-01-12 18:21:52,160][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_7.predict)=87.45%\n",
      "[ 2019-01-12 18:21:54,424][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_8.predict)=88.13%\n",
      "[ 2019-01-12 18:21:56,863][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_9.predict)=87.15%\n",
      "[ 2019-01-12 18:21:57,002][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_cv.predict)=87.70%\n",
      "[ 2019-01-12 18:21:57,004][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.test.predict)=88.59%\n",
      "[ 2019-01-12 18:21:58,046][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_0.predict)=85.75%\n",
      "[ 2019-01-12 18:21:59,200][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_1.predict)=87.99%\n",
      "[ 2019-01-12 18:22:00,354][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_2.predict)=86.64%\n",
      "[ 2019-01-12 18:22:01,541][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_3.predict)=87.75%\n",
      "[ 2019-01-12 18:22:02,704][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_4.predict)=87.98%\n",
      "[ 2019-01-12 18:22:03,873][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_5.predict)=88.13%\n",
      "[ 2019-01-12 18:22:05,048][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_6.predict)=87.08%\n",
      "[ 2019-01-12 18:22:06,155][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_7.predict)=88.96%\n",
      "[ 2019-01-12 18:22:07,309][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_8.predict)=87.38%\n",
      "[ 2019-01-12 18:22:08,424][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_9.predict)=87.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2019-01-12 18:22:08,545][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_cv.predict)=87.49%\n",
      "[ 2019-01-12 18:22:08,547][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.test.predict)=88.36%\n",
      "[ 2019-01-12 18:22:08,792][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_0.predict)=85.97%\n",
      "[ 2019-01-12 18:22:09,029][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_1.predict)=88.29%\n",
      "[ 2019-01-12 18:22:09,277][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_2.predict)=88.74%\n",
      "[ 2019-01-12 18:22:09,611][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_3.predict)=88.20%\n",
      "[ 2019-01-12 18:22:09,863][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_4.predict)=87.08%\n",
      "[ 2019-01-12 18:22:10,108][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_5.predict)=86.63%\n",
      "[ 2019-01-12 18:22:10,358][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_6.predict)=87.38%\n",
      "[ 2019-01-12 18:22:10,569][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_7.predict)=88.66%\n",
      "[ 2019-01-12 18:22:10,815][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_8.predict)=89.93%\n",
      "[ 2019-01-12 18:22:11,039][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_9.predict)=87.83%\n",
      "[ 2019-01-12 18:22:11,041][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_cv.predict)=87.87%\n",
      "[ 2019-01-12 18:22:11,042][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.test.predict)=88.19%\n",
      "[ 2019-01-12 18:22:11,044][cascade_classifier.calc_accuracy] Accuracy(layer_1 - train.classifier_average)=88.01%\n",
      "[ 2019-01-12 18:22:11,045][cascade_classifier.calc_accuracy] Accuracy(layer_1 - test.classifier_average)=88.47%\n",
      "[ 2019-01-12 18:22:11,133][cascade_classifier.fit_transform] [layer=2] look_indexs=[0], X_cur_train.shape=(13314, 16), X_cur_test.shape=(5706, 16)\n",
      "[ 2019-01-12 18:22:13,418][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_0.predict)=87.47%\n",
      "[ 2019-01-12 18:22:15,678][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_1.predict)=89.11%\n",
      "[ 2019-01-12 18:22:17,939][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_2.predict)=88.14%\n",
      "[ 2019-01-12 18:22:20,318][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_3.predict)=86.85%\n",
      "[ 2019-01-12 18:22:22,578][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_4.predict)=87.53%\n",
      "[ 2019-01-12 18:22:24,839][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_5.predict)=87.98%\n",
      "[ 2019-01-12 18:22:27,101][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_6.predict)=88.28%\n",
      "[ 2019-01-12 18:22:29,337][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_7.predict)=88.20%\n",
      "[ 2019-01-12 18:22:31,582][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_8.predict)=86.33%\n",
      "[ 2019-01-12 18:22:33,848][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_9.predict)=86.93%\n",
      "[ 2019-01-12 18:22:33,988][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_cv.predict)=87.68%\n",
      "[ 2019-01-12 18:22:33,992][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.test.predict)=88.47%\n",
      "[ 2019-01-12 18:22:35,044][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_0.predict)=88.22%\n",
      "[ 2019-01-12 18:22:36,199][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_1.predict)=86.34%\n",
      "[ 2019-01-12 18:22:37,339][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_2.predict)=88.36%\n",
      "[ 2019-01-12 18:22:38,486][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_3.predict)=86.33%\n",
      "[ 2019-01-12 18:22:39,858][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_4.predict)=87.30%\n",
      "[ 2019-01-12 18:22:41,134][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_5.predict)=86.78%\n",
      "[ 2019-01-12 18:22:42,286][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_6.predict)=88.20%\n",
      "[ 2019-01-12 18:22:43,435][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_7.predict)=88.66%\n",
      "[ 2019-01-12 18:22:44,616][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_8.predict)=87.90%\n",
      "[ 2019-01-12 18:22:45,806][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_9.predict)=87.90%\n",
      "[ 2019-01-12 18:22:45,923][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_cv.predict)=87.60%\n",
      "[ 2019-01-12 18:22:45,925][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.test.predict)=88.31%\n",
      "[ 2019-01-12 18:22:46,180][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_0.predict)=89.05%\n",
      "[ 2019-01-12 18:22:46,412][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_1.predict)=89.79%\n",
      "[ 2019-01-12 18:22:46,642][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_2.predict)=87.91%\n",
      "[ 2019-01-12 18:22:46,872][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_3.predict)=88.58%\n",
      "[ 2019-01-12 18:22:47,107][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_4.predict)=87.00%\n",
      "[ 2019-01-12 18:22:47,350][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_5.predict)=86.70%\n",
      "[ 2019-01-12 18:22:47,636][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_6.predict)=87.53%\n",
      "[ 2019-01-12 18:22:47,866][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_7.predict)=87.15%\n",
      "[ 2019-01-12 18:22:48,102][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_8.predict)=88.73%\n",
      "[ 2019-01-12 18:22:48,342][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_9.predict)=86.93%\n",
      "[ 2019-01-12 18:22:48,344][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_cv.predict)=87.94%\n",
      "[ 2019-01-12 18:22:48,346][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.test.predict)=88.52%\n",
      "[ 2019-01-12 18:22:48,349][cascade_classifier.calc_accuracy] Accuracy(layer_2 - train.classifier_average)=87.95%\n",
      "[ 2019-01-12 18:22:48,353][cascade_classifier.calc_accuracy] Accuracy(layer_2 - test.classifier_average)=88.54%\n",
      "[ 2019-01-12 18:22:48,355][cascade_classifier.fit_transform] [layer=3] look_indexs=[0], X_cur_train.shape=(13314, 16), X_cur_test.shape=(5706, 16)\n",
      "[ 2019-01-12 18:22:50,650][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_0.predict)=88.45%\n",
      "[ 2019-01-12 18:22:53,012][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_1.predict)=87.46%\n",
      "[ 2019-01-12 18:22:55,288][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_2.predict)=86.86%\n",
      "[ 2019-01-12 18:22:57,682][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_3.predict)=88.28%\n",
      "[ 2019-01-12 18:22:59,918][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_4.predict)=87.08%\n",
      "[ 2019-01-12 18:23:02,274][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_5.predict)=87.15%\n",
      "[ 2019-01-12 18:23:04,604][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_6.predict)=87.23%\n",
      "[ 2019-01-12 18:23:06,953][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_7.predict)=87.75%\n",
      "[ 2019-01-12 18:23:09,350][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_8.predict)=87.98%\n",
      "[ 2019-01-12 18:23:11,721][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_9.predict)=88.35%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2019-01-12 18:23:11,859][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_cv.predict)=87.66%\n",
      "[ 2019-01-12 18:23:11,862][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.test.predict)=88.42%\n",
      "[ 2019-01-12 18:23:12,898][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_0.predict)=87.47%\n",
      "[ 2019-01-12 18:23:14,057][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_1.predict)=87.99%\n",
      "[ 2019-01-12 18:23:15,216][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_2.predict)=86.64%\n",
      "[ 2019-01-12 18:23:16,383][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_3.predict)=87.68%\n",
      "[ 2019-01-12 18:23:17,619][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_4.predict)=87.00%\n",
      "[ 2019-01-12 18:23:18,741][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_5.predict)=88.35%\n",
      "[ 2019-01-12 18:23:19,890][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_6.predict)=87.68%\n",
      "[ 2019-01-12 18:23:21,024][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_7.predict)=88.05%\n",
      "[ 2019-01-12 18:23:22,172][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_8.predict)=86.33%\n",
      "[ 2019-01-12 18:23:23,300][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_9.predict)=87.08%\n",
      "[ 2019-01-12 18:23:23,423][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_cv.predict)=87.43%\n",
      "[ 2019-01-12 18:23:23,425][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.test.predict)=88.57%\n",
      "[ 2019-01-12 18:23:23,713][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_0.predict)=88.90%\n",
      "[ 2019-01-12 18:23:23,940][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_1.predict)=88.44%\n",
      "[ 2019-01-12 18:23:24,201][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_2.predict)=87.39%\n",
      "[ 2019-01-12 18:23:24,427][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_3.predict)=87.83%\n",
      "[ 2019-01-12 18:23:24,652][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_4.predict)=87.23%\n",
      "[ 2019-01-12 18:23:24,854][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_5.predict)=86.63%\n",
      "[ 2019-01-12 18:23:25,068][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_6.predict)=88.43%\n",
      "[ 2019-01-12 18:23:25,300][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_7.predict)=88.35%\n",
      "[ 2019-01-12 18:23:25,535][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_8.predict)=87.08%\n",
      "[ 2019-01-12 18:23:25,746][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_9.predict)=87.90%\n",
      "[ 2019-01-12 18:23:25,749][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_cv.predict)=87.82%\n",
      "[ 2019-01-12 18:23:25,752][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.test.predict)=88.31%\n",
      "[ 2019-01-12 18:23:25,757][cascade_classifier.calc_accuracy] Accuracy(layer_3 - train.classifier_average)=87.79%\n",
      "[ 2019-01-12 18:23:25,758][cascade_classifier.calc_accuracy] Accuracy(layer_3 - test.classifier_average)=88.59%\n",
      "[ 2019-01-12 18:23:25,760][cascade_classifier.fit_transform] [layer=4] look_indexs=[0], X_cur_train.shape=(13314, 16), X_cur_test.shape=(5706, 16)\n",
      "[ 2019-01-12 18:23:28,038][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_0.predict)=88.67%\n",
      "[ 2019-01-12 18:23:30,525][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_1.predict)=87.09%\n",
      "[ 2019-01-12 18:23:32,868][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_2.predict)=86.71%\n",
      "[ 2019-01-12 18:23:35,249][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_3.predict)=88.20%\n",
      "[ 2019-01-12 18:23:37,632][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_4.predict)=87.53%\n",
      "[ 2019-01-12 18:23:40,030][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_5.predict)=87.30%\n",
      "[ 2019-01-12 18:23:42,511][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_6.predict)=87.15%\n",
      "[ 2019-01-12 18:23:44,892][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_7.predict)=87.45%\n",
      "[ 2019-01-12 18:23:47,276][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_8.predict)=87.45%\n",
      "[ 2019-01-12 18:23:49,718][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_9.predict)=87.15%\n",
      "[ 2019-01-12 18:23:49,856][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_cv.predict)=87.47%\n",
      "[ 2019-01-12 18:23:49,859][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.test.predict)=88.43%\n",
      "[ 2019-01-12 18:23:50,953][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_0.predict)=88.67%\n",
      "[ 2019-01-12 18:23:52,138][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_1.predict)=87.69%\n",
      "[ 2019-01-12 18:23:53,282][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_2.predict)=86.34%\n",
      "[ 2019-01-12 18:23:54,565][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_3.predict)=88.20%\n",
      "[ 2019-01-12 18:23:55,682][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_4.predict)=87.00%\n",
      "[ 2019-01-12 18:23:56,832][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_5.predict)=87.90%\n",
      "[ 2019-01-12 18:23:57,951][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_6.predict)=87.60%\n",
      "[ 2019-01-12 18:23:59,100][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_7.predict)=86.93%\n",
      "[ 2019-01-12 18:24:00,230][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_8.predict)=87.45%\n",
      "[ 2019-01-12 18:24:01,368][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_9.predict)=86.70%\n",
      "[ 2019-01-12 18:24:01,482][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_cv.predict)=87.45%\n",
      "[ 2019-01-12 18:24:01,484][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.test.predict)=88.26%\n",
      "[ 2019-01-12 18:24:01,702][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_0.predict)=85.97%\n",
      "[ 2019-01-12 18:24:01,903][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_1.predict)=89.11%\n",
      "[ 2019-01-12 18:24:02,087][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_2.predict)=87.61%\n",
      "[ 2019-01-12 18:24:02,335][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_3.predict)=86.93%\n",
      "[ 2019-01-12 18:24:02,572][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_4.predict)=88.05%\n",
      "[ 2019-01-12 18:24:02,812][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_5.predict)=88.05%\n",
      "[ 2019-01-12 18:24:03,025][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_6.predict)=87.68%\n",
      "[ 2019-01-12 18:24:03,275][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_7.predict)=88.13%\n",
      "[ 2019-01-12 18:24:03,516][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_8.predict)=86.85%\n",
      "[ 2019-01-12 18:24:03,733][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_9.predict)=88.13%\n",
      "[ 2019-01-12 18:24:03,735][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_cv.predict)=87.65%\n",
      "[ 2019-01-12 18:24:03,736][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.test.predict)=88.38%\n",
      "[ 2019-01-12 18:24:03,738][cascade_classifier.calc_accuracy] Accuracy(layer_4 - train.classifier_average)=87.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2019-01-12 18:24:03,739][cascade_classifier.calc_accuracy] Accuracy(layer_4 - test.classifier_average)=88.36%\n",
      "[ 2019-01-12 18:24:03,740][cascade_classifier.fit_transform] [Result][Optimal Level Detected] opt_layer_num=2, accuracy_train=88.01%, accuracy_test=88.47%\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "# X_enc is the concatenated predict_proba result of each estimators of the last layer of the GCForest model\n",
    "    # X_enc.shape =\n",
    "    #   (n_datas, n_estimators * n_classes): If cascade is provided\n",
    "    #   (n_datas, n_estimators * n_classes, dimX, dimY): If only finegrained part is provided\n",
    "    # You can also pass X_test, y_test to fit_transform method, then the accracy on test data will be logged when training.\n",
    "X_train_enc, X_test_enc = gc.fit_transform(X_train, y_train, X_test=X_test, y_test=y_test)\n",
    "    # WARNING: if you set gc.set_keep_model_in_mem(True), you would have to use\n",
    "    # gc.fit_transform(X_train, y_train, X_test=X_test, y_test=y_test) to evaluate your model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2019-01-12 18:24:03,791][cascade_classifier.transform] X_groups_test.shape=[(5706, 10)]\n",
      "[ 2019-01-12 18:24:03,794][cascade_classifier.transform] group_dims=[10]\n",
      "[ 2019-01-12 18:24:03,802][cascade_classifier.transform] X_test.shape=(5706, 10)\n",
      "[ 2019-01-12 18:24:03,811][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(5706, 10)\n",
      "[ 2019-01-12 18:24:06,261][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(5706, 16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of GCForest = 88.468279 %\n",
      "(' Time ', '191.519', ' seconds')\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "y_pred = gc.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy of GCForest = {:.6f} %\".format(acc * 100))\n",
    "\n",
    "tt = time() - t0     \n",
    "print (\" Time \",format(round(tt,3)),\" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1566  451]\n",
      " [ 207 3482]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.88      0.78      0.83      2017\n",
      "        1.0       0.89      0.94      0.91      3689\n",
      "\n",
      "avg / total       0.88      0.88      0.88      5706\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Matrix de confusion\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "11982/11982 [==============================] - 7s 563us/step - loss: 0.5836 - acc: 0.7458\n",
      "Epoch 2/50\n",
      "11982/11982 [==============================] - 6s 487us/step - loss: 0.4496 - acc: 0.7919\n",
      "Epoch 3/50\n",
      "11982/11982 [==============================] - 6s 475us/step - loss: 0.4247 - acc: 0.8093\n",
      "Epoch 4/50\n",
      "11982/11982 [==============================] - 6s 481us/step - loss: 0.4096 - acc: 0.8146\n",
      "Epoch 5/50\n",
      "11982/11982 [==============================] - 6s 480us/step - loss: 0.4085 - acc: 0.8159\n",
      "Epoch 6/50\n",
      "11982/11982 [==============================] - 6s 476us/step - loss: 0.4015 - acc: 0.8180\n",
      "Epoch 7/50\n",
      "11982/11982 [==============================] - 6s 481us/step - loss: 0.3955 - acc: 0.8252\n",
      "Epoch 8/50\n",
      "11982/11982 [==============================] - 6s 477us/step - loss: 0.3968 - acc: 0.8196\n",
      "Epoch 9/50\n",
      "11982/11982 [==============================] - 6s 478us/step - loss: 0.3894 - acc: 0.8257\n",
      "Epoch 10/50\n",
      "11982/11982 [==============================] - 6s 479us/step - loss: 0.3831 - acc: 0.8258\n",
      "Epoch 11/50\n",
      "11982/11982 [==============================] - 6s 472us/step - loss: 0.3844 - acc: 0.8258\n",
      "Epoch 12/50\n",
      "11982/11982 [==============================] - 6s 477us/step - loss: 0.3803 - acc: 0.8266\n",
      "Epoch 13/50\n",
      "11982/11982 [==============================] - 6s 484us/step - loss: 0.3768 - acc: 0.8292\n",
      "Epoch 14/50\n",
      "11982/11982 [==============================] - 6s 479us/step - loss: 0.3714 - acc: 0.8354\n",
      "Epoch 15/50\n",
      "11982/11982 [==============================] - 6s 490us/step - loss: 0.3730 - acc: 0.8338\n",
      "Epoch 16/50\n",
      "11982/11982 [==============================] - 6s 530us/step - loss: 0.3684 - acc: 0.8357 1\n",
      "Epoch 17/50\n",
      "11982/11982 [==============================] - 9s 743us/step - loss: 0.3660 - acc: 0.8375\n",
      "Epoch 18/50\n",
      "11982/11982 [==============================] - 9s 737us/step - loss: 0.3628 - acc: 0.8399\n",
      "Epoch 19/50\n",
      "11982/11982 [==============================] - 8s 628us/step - loss: 0.3600 - acc: 0.8433\n",
      "Epoch 20/50\n",
      "11982/11982 [==============================] - 7s 550us/step - loss: 0.3620 - acc: 0.8406\n",
      "Epoch 21/50\n",
      "11982/11982 [==============================] - 6s 464us/step - loss: 0.3563 - acc: 0.8404\n",
      "Epoch 22/50\n",
      "11982/11982 [==============================] - 6s 461us/step - loss: 0.3510 - acc: 0.8465\n",
      "Epoch 23/50\n",
      "11982/11982 [==============================] - 6s 467us/step - loss: 0.3499 - acc: 0.8446\n",
      "Epoch 24/50\n",
      "11982/11982 [==============================] - 5s 435us/step - loss: 0.3478 - acc: 0.8464\n",
      "Epoch 25/50\n",
      "11982/11982 [==============================] - 5s 430us/step - loss: 0.3490 - acc: 0.8468\n",
      "Epoch 26/50\n",
      "11982/11982 [==============================] - 6s 486us/step - loss: 0.3466 - acc: 0.8475\n",
      "Epoch 27/50\n",
      "11982/11982 [==============================] - 7s 555us/step - loss: 0.3423 - acc: 0.8504\n",
      "Epoch 28/50\n",
      "11982/11982 [==============================] - 5s 455us/step - loss: 0.3387 - acc: 0.8494\n",
      "Epoch 29/50\n",
      "11982/11982 [==============================] - 6s 464us/step - loss: 0.3386 - acc: 0.8512\n",
      "Epoch 30/50\n",
      "11982/11982 [==============================] - 5s 430us/step - loss: 0.3336 - acc: 0.8522\n",
      "Epoch 31/50\n",
      "11982/11982 [==============================] - 6s 500us/step - loss: 0.3324 - acc: 0.8515\n",
      "Epoch 32/50\n",
      "11982/11982 [==============================] - 6s 529us/step - loss: 0.3338 - acc: 0.8518\n",
      "Epoch 33/50\n",
      "11982/11982 [==============================] - 7s 547us/step - loss: 0.3287 - acc: 0.8546\n",
      "Epoch 34/50\n",
      "11982/11982 [==============================] - 8s 627us/step - loss: 0.3281 - acc: 0.8545\n",
      "Epoch 35/50\n",
      "11982/11982 [==============================] - 7s 579us/step - loss: 0.3267 - acc: 0.8554\n",
      "Epoch 36/50\n",
      "11982/11982 [==============================] - 6s 520us/step - loss: 0.3210 - acc: 0.8559\n",
      "Epoch 37/50\n",
      "11982/11982 [==============================] - 6s 492us/step - loss: 0.3207 - acc: 0.8566\n",
      "Epoch 38/50\n",
      "11982/11982 [==============================] - 6s 465us/step - loss: 0.3213 - acc: 0.8590\n",
      "Epoch 39/50\n",
      "11982/11982 [==============================] - 6s 523us/step - loss: 0.3139 - acc: 0.8600\n",
      "Epoch 40/50\n",
      "11982/11982 [==============================] - 6s 517us/step - loss: 0.3172 - acc: 0.8609\n",
      "Epoch 41/50\n",
      "11982/11982 [==============================] - 6s 542us/step - loss: 0.3117 - acc: 0.8596\n",
      "Epoch 42/50\n",
      "11982/11982 [==============================] - 6s 495us/step - loss: 0.3109 - acc: 0.8624\n",
      "Epoch 43/50\n",
      "11982/11982 [==============================] - 6s 472us/step - loss: 0.3192 - acc: 0.8622\n",
      "Epoch 44/50\n",
      "11982/11982 [==============================] - 7s 573us/step - loss: 0.3093 - acc: 0.8644\n",
      "Epoch 45/50\n",
      "11982/11982 [==============================] - 10s 794us/step - loss: 0.3037 - acc: 0.8692\n",
      "Epoch 46/50\n",
      "11982/11982 [==============================] - 9s 789us/step - loss: 0.3019 - acc: 0.8665\n",
      "Epoch 47/50\n",
      "11982/11982 [==============================] - 7s 602us/step - loss: 0.2992 - acc: 0.8696\n",
      "Epoch 48/50\n",
      "11982/11982 [==============================] - 6s 480us/step - loss: 0.2988 - acc: 0.8660\n",
      "Epoch 49/50\n",
      "11982/11982 [==============================] - 5s 456us/step - loss: 0.3009 - acc: 0.8664\n",
      "Epoch 50/50\n",
      "11982/11982 [==============================] - 6s 484us/step - loss: 0.3007 - acc: 0.8658\n",
      "1332/1332 [==============================] - 0s 134us/step\n",
      "Epoch 1/50\n",
      "11982/11982 [==============================] - 6s 541us/step - loss: 0.5997 - acc: 0.7459\n",
      "Epoch 2/50\n",
      "11982/11982 [==============================] - 6s 481us/step - loss: 0.4500 - acc: 0.7968\n",
      "Epoch 3/50\n",
      "11982/11982 [==============================] - 5s 440us/step - loss: 0.4330 - acc: 0.8022\n",
      "Epoch 4/50\n",
      "11982/11982 [==============================] - 6s 480us/step - loss: 0.4146 - acc: 0.8156\n",
      "Epoch 5/50\n",
      "11982/11982 [==============================] - 5s 439us/step - loss: 0.4107 - acc: 0.8151\n",
      "Epoch 6/50\n",
      "11982/11982 [==============================] - 6s 531us/step - loss: 0.4027 - acc: 0.8182\n",
      "Epoch 7/50\n",
      "11982/11982 [==============================] - 5s 442us/step - loss: 0.3984 - acc: 0.8204\n",
      "Epoch 8/50\n",
      "11982/11982 [==============================] - 6s 468us/step - loss: 0.3962 - acc: 0.8212\n",
      "Epoch 9/50\n",
      "11982/11982 [==============================] - 5s 441us/step - loss: 0.3892 - acc: 0.8264\n",
      "Epoch 10/50\n",
      "11982/11982 [==============================] - 6s 466us/step - loss: 0.3883 - acc: 0.8232\n",
      "Epoch 11/50\n",
      "11982/11982 [==============================] - 6s 461us/step - loss: 0.3831 - acc: 0.8248\n",
      "Epoch 12/50\n",
      "11982/11982 [==============================] - 5s 458us/step - loss: 0.3796 - acc: 0.8290\n",
      "Epoch 13/50\n",
      "11982/11982 [==============================] - 6s 471us/step - loss: 0.3769 - acc: 0.8327\n",
      "Epoch 14/50\n",
      "11982/11982 [==============================] - 6s 465us/step - loss: 0.3726 - acc: 0.8326\n",
      "Epoch 15/50\n",
      "11982/11982 [==============================] - 6s 464us/step - loss: 0.3697 - acc: 0.8355\n",
      "Epoch 16/50\n",
      "11982/11982 [==============================] - 6s 460us/step - loss: 0.3675 - acc: 0.8368\n",
      "Epoch 17/50\n",
      "11982/11982 [==============================] - 5s 443us/step - loss: 0.3652 - acc: 0.8373\n",
      "Epoch 18/50\n",
      "11982/11982 [==============================] - 5s 430us/step - loss: 0.3652 - acc: 0.8411\n",
      "Epoch 19/50\n",
      "11982/11982 [==============================] - 5s 433us/step - loss: 0.3572 - acc: 0.8416\n",
      "Epoch 20/50\n",
      "11982/11982 [==============================] - 5s 426us/step - loss: 0.3544 - acc: 0.8434\n",
      "Epoch 21/50\n",
      "11982/11982 [==============================] - 5s 458us/step - loss: 0.3541 - acc: 0.8453\n",
      "Epoch 22/50\n",
      "11982/11982 [==============================] - 6s 476us/step - loss: 0.3493 - acc: 0.8486\n",
      "Epoch 23/50\n",
      "11982/11982 [==============================] - 6s 495us/step - loss: 0.3497 - acc: 0.8453\n",
      "Epoch 24/50\n",
      "11982/11982 [==============================] - 5s 425us/step - loss: 0.3490 - acc: 0.8465\n",
      "Epoch 25/50\n",
      "11982/11982 [==============================] - 5s 426us/step - loss: 0.3443 - acc: 0.8480\n",
      "Epoch 26/50\n",
      "11982/11982 [==============================] - 5s 444us/step - loss: 0.3445 - acc: 0.8518\n",
      "Epoch 27/50\n",
      "11982/11982 [==============================] - 6s 482us/step - loss: 0.3382 - acc: 0.8494\n",
      "Epoch 28/50\n",
      "11982/11982 [==============================] - 5s 455us/step - loss: 0.3366 - acc: 0.8521\n",
      "Epoch 29/50\n",
      "11982/11982 [==============================] - 5s 452us/step - loss: 0.3354 - acc: 0.8544\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11982/11982 [==============================] - 6s 470us/step - loss: 0.3365 - acc: 0.8527\n",
      "Epoch 31/50\n",
      "11982/11982 [==============================] - 6s 482us/step - loss: 0.3328 - acc: 0.8544\n",
      "Epoch 32/50\n",
      "11982/11982 [==============================] - 6s 496us/step - loss: 0.3307 - acc: 0.8543\n",
      "Epoch 33/50\n",
      "11982/11982 [==============================] - 7s 567us/step - loss: 0.3272 - acc: 0.8577\n",
      "Epoch 34/50\n",
      "11982/11982 [==============================] - 6s 497us/step - loss: 0.3281 - acc: 0.8579\n",
      "Epoch 35/50\n",
      "11982/11982 [==============================] - 7s 543us/step - loss: 0.3235 - acc: 0.8595\n",
      "Epoch 36/50\n",
      "11982/11982 [==============================] - 6s 476us/step - loss: 0.3206 - acc: 0.8574\n",
      "Epoch 37/50\n",
      "11982/11982 [==============================] - 6s 496us/step - loss: 0.3218 - acc: 0.8593\n",
      "Epoch 38/50\n",
      "11982/11982 [==============================] - 7s 553us/step - loss: 0.3200 - acc: 0.8592\n",
      "Epoch 39/50\n",
      "11982/11982 [==============================] - 6s 483us/step - loss: 0.3148 - acc: 0.8625\n",
      "Epoch 40/50\n",
      "11982/11982 [==============================] - 6s 462us/step - loss: 0.3180 - acc: 0.8613\n",
      "Epoch 41/50\n",
      "11982/11982 [==============================] - 6s 472us/step - loss: 0.3143 - acc: 0.8616\n",
      "Epoch 42/50\n",
      "11982/11982 [==============================] - 6s 459us/step - loss: 0.3117 - acc: 0.8640\n",
      "Epoch 43/50\n",
      "11982/11982 [==============================] - 5s 451us/step - loss: 0.3080 - acc: 0.8635\n",
      "Epoch 44/50\n",
      "11982/11982 [==============================] - 7s 594us/step - loss: 0.3059 - acc: 0.8684\n",
      "Epoch 45/50\n",
      "11982/11982 [==============================] - 6s 468us/step - loss: 0.3051 - acc: 0.8649\n",
      "Epoch 46/50\n",
      "11982/11982 [==============================] - 6s 505us/step - loss: 0.3068 - acc: 0.8667\n",
      "Epoch 47/50\n",
      "11982/11982 [==============================] - 6s 524us/step - loss: 0.3015 - acc: 0.8686\n",
      "Epoch 48/50\n",
      "11982/11982 [==============================] - 7s 572us/step - loss: 0.3023 - acc: 0.8684\n",
      "Epoch 49/50\n",
      "11982/11982 [==============================] - 5s 436us/step - loss: 0.3010 - acc: 0.8691\n",
      "Epoch 50/50\n",
      "11982/11982 [==============================] - 6s 473us/step - loss: 0.2951 - acc: 0.8716\n",
      "1332/1332 [==============================] - 0s 126us/step\n",
      "Epoch 1/50\n",
      "11982/11982 [==============================] - 5s 449us/step - loss: 0.5721 - acc: 0.7490\n",
      "Epoch 2/50\n",
      "11982/11982 [==============================] - 5s 440us/step - loss: 0.4482 - acc: 0.7959\n",
      "Epoch 3/50\n",
      "11982/11982 [==============================] - 6s 527us/step - loss: 0.4292 - acc: 0.8080\n",
      "Epoch 4/50\n",
      "11982/11982 [==============================] - 5s 457us/step - loss: 0.4167 - acc: 0.8127\n",
      "Epoch 5/50\n",
      "11982/11982 [==============================] - 5s 449us/step - loss: 0.4039 - acc: 0.8181\n",
      "Epoch 6/50\n",
      "11982/11982 [==============================] - 6s 495us/step - loss: 0.4012 - acc: 0.8191\n",
      "Epoch 7/50\n",
      "11982/11982 [==============================] - 6s 501us/step - loss: 0.3992 - acc: 0.8200\n",
      "Epoch 8/50\n",
      "11982/11982 [==============================] - 5s 457us/step - loss: 0.3934 - acc: 0.8214\n",
      "Epoch 9/50\n",
      "11982/11982 [==============================] - 6s 485us/step - loss: 0.3879 - acc: 0.8250\n",
      "Epoch 10/50\n",
      "11982/11982 [==============================] - 5s 442us/step - loss: 0.3874 - acc: 0.8241\n",
      "Epoch 11/50\n",
      "11982/11982 [==============================] - 6s 515us/step - loss: 0.3810 - acc: 0.8292\n",
      "Epoch 12/50\n",
      "11982/11982 [==============================] - 6s 494us/step - loss: 0.3788 - acc: 0.8330\n",
      "Epoch 13/50\n",
      "11982/11982 [==============================] - 6s 497us/step - loss: 0.3788 - acc: 0.8312\n",
      "Epoch 14/50\n",
      "11982/11982 [==============================] - 5s 455us/step - loss: 0.3730 - acc: 0.8357\n",
      "Epoch 15/50\n",
      "11982/11982 [==============================] - 5s 453us/step - loss: 0.3685 - acc: 0.8383\n",
      "Epoch 16/50\n",
      "11982/11982 [==============================] - 6s 471us/step - loss: 0.3648 - acc: 0.8360\n",
      "Epoch 17/50\n",
      "11982/11982 [==============================] - 5s 446us/step - loss: 0.3663 - acc: 0.8373\n",
      "Epoch 18/50\n",
      "11982/11982 [==============================] - 6s 465us/step - loss: 0.3635 - acc: 0.8352\n",
      "Epoch 19/50\n",
      "11982/11982 [==============================] - 6s 504us/step - loss: 0.3590 - acc: 0.8398\n",
      "Epoch 20/50\n",
      "11982/11982 [==============================] - 6s 494us/step - loss: 0.3538 - acc: 0.8441\n",
      "Epoch 21/50\n",
      "11982/11982 [==============================] - 6s 473us/step - loss: 0.3502 - acc: 0.8456\n",
      "Epoch 22/50\n",
      "11982/11982 [==============================] - 5s 443us/step - loss: 0.3476 - acc: 0.8465\n",
      "Epoch 23/50\n",
      "11982/11982 [==============================] - 6s 493us/step - loss: 0.3458 - acc: 0.8482\n",
      "Epoch 24/50\n",
      "11982/11982 [==============================] - 6s 477us/step - loss: 0.3484 - acc: 0.8449\n",
      "Epoch 25/50\n",
      "11982/11982 [==============================] - 6s 483us/step - loss: 0.3433 - acc: 0.8499\n",
      "Epoch 26/50\n",
      "11982/11982 [==============================] - 7s 566us/step - loss: 0.3434 - acc: 0.8484\n",
      "Epoch 27/50\n",
      "11982/11982 [==============================] - 6s 532us/step - loss: 0.3392 - acc: 0.8523\n",
      "Epoch 28/50\n",
      "11982/11982 [==============================] - 5s 453us/step - loss: 0.3394 - acc: 0.8508\n",
      "Epoch 29/50\n",
      "11982/11982 [==============================] - 6s 484us/step - loss: 0.3318 - acc: 0.8535\n",
      "Epoch 30/50\n",
      "11982/11982 [==============================] - 6s 506us/step - loss: 0.3321 - acc: 0.8540\n",
      "Epoch 31/50\n",
      "11982/11982 [==============================] - 7s 602us/step - loss: 0.3291 - acc: 0.8535\n",
      "Epoch 32/50\n",
      "11982/11982 [==============================] - 8s 648us/step - loss: 0.3274 - acc: 0.8543\n",
      "Epoch 33/50\n",
      "11982/11982 [==============================] - 8s 702us/step - loss: 0.3299 - acc: 0.8550\n",
      "Epoch 34/50\n",
      "11982/11982 [==============================] - 6s 518us/step - loss: 0.3250 - acc: 0.8570\n",
      "Epoch 35/50\n",
      "11982/11982 [==============================] - 6s 486us/step - loss: 0.3232 - acc: 0.8575\n",
      "Epoch 36/50\n",
      "11982/11982 [==============================] - 6s 467us/step - loss: 0.3217 - acc: 0.8584\n",
      "Epoch 37/50\n",
      "11982/11982 [==============================] - 7s 559us/step - loss: 0.3203 - acc: 0.8617\n",
      "Epoch 38/50\n",
      "11982/11982 [==============================] - 6s 515us/step - loss: 0.3187 - acc: 0.8581\n",
      "Epoch 39/50\n",
      "11982/11982 [==============================] - 7s 559us/step - loss: 0.3132 - acc: 0.8640\n",
      "Epoch 40/50\n",
      "11982/11982 [==============================] - 8s 653us/step - loss: 0.3112 - acc: 0.8648\n",
      "Epoch 41/50\n",
      "11982/11982 [==============================] - 6s 495us/step - loss: 0.3125 - acc: 0.8632\n",
      "Epoch 42/50\n",
      "11982/11982 [==============================] - 5s 457us/step - loss: 0.3069 - acc: 0.8650\n",
      "Epoch 43/50\n",
      "11982/11982 [==============================] - 6s 476us/step - loss: 0.3026 - acc: 0.8671\n",
      "Epoch 44/50\n",
      "11982/11982 [==============================] - 7s 581us/step - loss: 0.3036 - acc: 0.8665\n",
      "Epoch 45/50\n",
      "11982/11982 [==============================] - 6s 514us/step - loss: 0.3013 - acc: 0.8702\n",
      "Epoch 46/50\n",
      "11982/11982 [==============================] - 6s 481us/step - loss: 0.3026 - acc: 0.8677\n",
      "Epoch 47/50\n",
      "11982/11982 [==============================] - 6s 484us/step - loss: 0.3063 - acc: 0.8661\n",
      "Epoch 48/50\n",
      "11982/11982 [==============================] - 6s 487us/step - loss: 0.3016 - acc: 0.8693\n",
      "Epoch 49/50\n",
      "11982/11982 [==============================] - 5s 457us/step - loss: 0.2951 - acc: 0.8708\n",
      "Epoch 50/50\n",
      "11982/11982 [==============================] - 6s 496us/step - loss: 0.2928 - acc: 0.8721\n",
      "1332/1332 [==============================] - 0s 157us/step\n",
      "Epoch 1/50\n",
      "11982/11982 [==============================] - 7s 580us/step - loss: 0.5634 - acc: 0.7452\n",
      "Epoch 2/50\n",
      "11982/11982 [==============================] - 7s 574us/step - loss: 0.4496 - acc: 0.7957\n",
      "Epoch 3/50\n",
      "11982/11982 [==============================] - 6s 519us/step - loss: 0.4271 - acc: 0.8073\n",
      "Epoch 4/50\n",
      "11982/11982 [==============================] - 6s 463us/step - loss: 0.4138 - acc: 0.8158\n",
      "Epoch 5/50\n",
      "11982/11982 [==============================] - 6s 481us/step - loss: 0.4123 - acc: 0.8157\n",
      "Epoch 6/50\n",
      "11982/11982 [==============================] - 6s 486us/step - loss: 0.4089 - acc: 0.8172\n",
      "Epoch 7/50\n",
      "11982/11982 [==============================] - 8s 657us/step - loss: 0.3987 - acc: 0.8193\n",
      "Epoch 8/50\n",
      "11982/11982 [==============================] - 6s 540us/step - loss: 0.3946 - acc: 0.8242\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11982/11982 [==============================] - 7s 588us/step - loss: 0.3902 - acc: 0.8272\n",
      "Epoch 10/50\n",
      "11982/11982 [==============================] - 8s 657us/step - loss: 0.3899 - acc: 0.8236\n",
      "Epoch 11/50\n",
      "11982/11982 [==============================] - 7s 547us/step - loss: 0.3863 - acc: 0.8282\n",
      "Epoch 12/50\n",
      "11982/11982 [==============================] - 7s 553us/step - loss: 0.3838 - acc: 0.8278\n",
      "Epoch 13/50\n",
      "11982/11982 [==============================] - 7s 613us/step - loss: 0.3777 - acc: 0.8294\n",
      "Epoch 14/50\n",
      "11982/11982 [==============================] - 6s 522us/step - loss: 0.3753 - acc: 0.8305\n",
      "Epoch 15/50\n",
      "11982/11982 [==============================] - 6s 489us/step - loss: 0.3722 - acc: 0.8345\n",
      "Epoch 16/50\n",
      "11982/11982 [==============================] - 7s 618us/step - loss: 0.3701 - acc: 0.8350\n",
      "Epoch 17/50\n",
      "11982/11982 [==============================] - 6s 542us/step - loss: 0.3684 - acc: 0.8368\n",
      "Epoch 18/50\n",
      "11982/11982 [==============================] - 6s 478us/step - loss: 0.3628 - acc: 0.8407\n",
      "Epoch 19/50\n",
      "11982/11982 [==============================] - 6s 491us/step - loss: 0.3578 - acc: 0.8410\n",
      "Epoch 20/50\n",
      "11982/11982 [==============================] - 6s 466us/step - loss: 0.3607 - acc: 0.8429\n",
      "Epoch 21/50\n",
      "11982/11982 [==============================] - 6s 477us/step - loss: 0.3581 - acc: 0.8422\n",
      "Epoch 22/50\n",
      "11982/11982 [==============================] - 5s 432us/step - loss: 0.3571 - acc: 0.8413\n",
      "Epoch 23/50\n",
      "11982/11982 [==============================] - 6s 462us/step - loss: 0.3535 - acc: 0.8443\n",
      "Epoch 24/50\n",
      "11982/11982 [==============================] - 6s 538us/step - loss: 0.3526 - acc: 0.8474\n",
      "Epoch 25/50\n",
      "11982/11982 [==============================] - 6s 505us/step - loss: 0.3477 - acc: 0.8471\n",
      "Epoch 26/50\n",
      "11982/11982 [==============================] - 6s 487us/step - loss: 0.3505 - acc: 0.8441\n",
      "Epoch 27/50\n",
      "11982/11982 [==============================] - 6s 489us/step - loss: 0.3426 - acc: 0.8490\n",
      "Epoch 28/50\n",
      "11982/11982 [==============================] - 5s 459us/step - loss: 0.3407 - acc: 0.8526\n",
      "Epoch 29/50\n",
      "11982/11982 [==============================] - 6s 514us/step - loss: 0.3363 - acc: 0.8536\n",
      "Epoch 30/50\n",
      "11982/11982 [==============================] - 5s 440us/step - loss: 0.3342 - acc: 0.8519\n",
      "Epoch 31/50\n",
      "11982/11982 [==============================] - 6s 519us/step - loss: 0.3358 - acc: 0.8523 1s - los\n",
      "Epoch 32/50\n",
      "11982/11982 [==============================] - 6s 462us/step - loss: 0.3333 - acc: 0.8537\n",
      "Epoch 33/50\n",
      "11982/11982 [==============================] - 5s 435us/step - loss: 0.3312 - acc: 0.8535\n",
      "Epoch 34/50\n",
      "11982/11982 [==============================] - 6s 480us/step - loss: 0.3287 - acc: 0.8560\n",
      "Epoch 35/50\n",
      "11982/11982 [==============================] - 7s 583us/step - loss: 0.3249 - acc: 0.8597\n",
      "Epoch 36/50\n",
      "11982/11982 [==============================] - 7s 548us/step - loss: 0.3220 - acc: 0.8578\n",
      "Epoch 37/50\n",
      "11982/11982 [==============================] - 6s 529us/step - loss: 0.3221 - acc: 0.8587\n",
      "Epoch 38/50\n",
      "11982/11982 [==============================] - 7s 575us/step - loss: 0.3195 - acc: 0.8603\n",
      "Epoch 39/50\n",
      "11982/11982 [==============================] - 7s 543us/step - loss: 0.3146 - acc: 0.8630\n",
      "Epoch 40/50\n",
      "11982/11982 [==============================] - 6s 515us/step - loss: 0.3156 - acc: 0.8609\n",
      "Epoch 41/50\n",
      "11982/11982 [==============================] - 6s 513us/step - loss: 0.3089 - acc: 0.8647\n",
      "Epoch 42/50\n",
      "11982/11982 [==============================] - 8s 632us/step - loss: 0.3068 - acc: 0.8647\n",
      "Epoch 43/50\n",
      "11982/11982 [==============================] - 7s 623us/step - loss: 0.3082 - acc: 0.8660\n",
      "Epoch 44/50\n",
      "11982/11982 [==============================] - 8s 631us/step - loss: 0.3099 - acc: 0.8655\n",
      "Epoch 45/50\n",
      "11982/11982 [==============================] - 6s 517us/step - loss: 0.3024 - acc: 0.8720\n",
      "Epoch 46/50\n",
      "11982/11982 [==============================] - 6s 510us/step - loss: 0.3041 - acc: 0.8661\n",
      "Epoch 47/50\n",
      "11982/11982 [==============================] - 7s 600us/step - loss: 0.2980 - acc: 0.8691 0s - loss: 0.2983 - acc: 0.8\n",
      "Epoch 48/50\n",
      "11982/11982 [==============================] - 8s 630us/step - loss: 0.2949 - acc: 0.8709\n",
      "Epoch 49/50\n",
      "11982/11982 [==============================] - 6s 539us/step - loss: 0.2953 - acc: 0.8699\n",
      "Epoch 50/50\n",
      "11982/11982 [==============================] - 7s 590us/step - loss: 0.2960 - acc: 0.8713\n",
      "1332/1332 [==============================] - 0s 209us/step\n",
      "Epoch 1/50\n",
      "11983/11983 [==============================] - 8s 640us/step - loss: 0.6516 - acc: 0.7446\n",
      "Epoch 2/50\n",
      "11983/11983 [==============================] - 6s 501us/step - loss: 0.4497 - acc: 0.7958\n",
      "Epoch 3/50\n",
      "11983/11983 [==============================] - 6s 495us/step - loss: 0.4223 - acc: 0.8079\n",
      "Epoch 4/50\n",
      "11983/11983 [==============================] - 7s 571us/step - loss: 0.4120 - acc: 0.8132\n",
      "Epoch 5/50\n",
      "11983/11983 [==============================] - 6s 508us/step - loss: 0.4055 - acc: 0.8167\n",
      "Epoch 6/50\n",
      "11983/11983 [==============================] - 6s 520us/step - loss: 0.4033 - acc: 0.8183\n",
      "Epoch 7/50\n",
      "11983/11983 [==============================] - 6s 531us/step - loss: 0.3977 - acc: 0.8226\n",
      "Epoch 8/50\n",
      "11983/11983 [==============================] - 6s 514us/step - loss: 0.3918 - acc: 0.8245\n",
      "Epoch 9/50\n",
      "11983/11983 [==============================] - 6s 525us/step - loss: 0.3893 - acc: 0.8251\n",
      "Epoch 10/50\n",
      "11983/11983 [==============================] - 6s 499us/step - loss: 0.3844 - acc: 0.8268\n",
      "Epoch 11/50\n",
      "11983/11983 [==============================] - 6s 495us/step - loss: 0.3853 - acc: 0.8273\n",
      "Epoch 12/50\n",
      "11983/11983 [==============================] - 6s 484us/step - loss: 0.3780 - acc: 0.8310\n",
      "Epoch 13/50\n",
      "11983/11983 [==============================] - 7s 563us/step - loss: 0.3795 - acc: 0.8311\n",
      "Epoch 14/50\n",
      "11983/11983 [==============================] - 6s 506us/step - loss: 0.3772 - acc: 0.8335\n",
      "Epoch 15/50\n",
      "11983/11983 [==============================] - 7s 573us/step - loss: 0.3734 - acc: 0.8319\n",
      "Epoch 16/50\n",
      "11983/11983 [==============================] - 7s 617us/step - loss: 0.3683 - acc: 0.8359\n",
      "Epoch 17/50\n",
      "11983/11983 [==============================] - 6s 519us/step - loss: 0.3671 - acc: 0.8381\n",
      "Epoch 18/50\n",
      "11983/11983 [==============================] - 6s 504us/step - loss: 0.3626 - acc: 0.8387\n",
      "Epoch 19/50\n",
      "11983/11983 [==============================] - 6s 485us/step - loss: 0.3594 - acc: 0.8399\n",
      "Epoch 20/50\n",
      "11983/11983 [==============================] - 6s 475us/step - loss: 0.3601 - acc: 0.8414\n",
      "Epoch 21/50\n",
      "11983/11983 [==============================] - 6s 504us/step - loss: 0.3544 - acc: 0.8419\n",
      "Epoch 22/50\n",
      "11983/11983 [==============================] - 6s 540us/step - loss: 0.3547 - acc: 0.8441\n",
      "Epoch 23/50\n",
      "11983/11983 [==============================] - 8s 632us/step - loss: 0.3531 - acc: 0.8446\n",
      "Epoch 24/50\n",
      "11983/11983 [==============================] - 8s 677us/step - loss: 0.3473 - acc: 0.8475\n",
      "Epoch 25/50\n",
      "11983/11983 [==============================] - 7s 595us/step - loss: 0.3483 - acc: 0.8483\n",
      "Epoch 26/50\n",
      "11983/11983 [==============================] - 6s 530us/step - loss: 0.3458 - acc: 0.8508\n",
      "Epoch 27/50\n",
      "11983/11983 [==============================] - 7s 567us/step - loss: 0.3413 - acc: 0.8509\n",
      "Epoch 28/50\n",
      "11983/11983 [==============================] - 6s 475us/step - loss: 0.3420 - acc: 0.8505\n",
      "Epoch 29/50\n",
      "11983/11983 [==============================] - 6s 489us/step - loss: 0.3384 - acc: 0.8524\n",
      "Epoch 30/50\n",
      "11983/11983 [==============================] - 6s 497us/step - loss: 0.3358 - acc: 0.8525\n",
      "Epoch 31/50\n",
      "11983/11983 [==============================] - 6s 462us/step - loss: 0.3333 - acc: 0.8518\n",
      "Epoch 32/50\n",
      "11983/11983 [==============================] - 6s 536us/step - loss: 0.3325 - acc: 0.8556\n",
      "Epoch 33/50\n",
      "11983/11983 [==============================] - 6s 482us/step - loss: 0.3315 - acc: 0.8536\n",
      "Epoch 34/50\n",
      "11983/11983 [==============================] - 5s 439us/step - loss: 0.3268 - acc: 0.8562\n",
      "Epoch 35/50\n",
      "11983/11983 [==============================] - 6s 465us/step - loss: 0.3240 - acc: 0.8624\n",
      "Epoch 36/50\n",
      "11983/11983 [==============================] - 5s 441us/step - loss: 0.3226 - acc: 0.8621\n",
      "Epoch 37/50\n",
      "11983/11983 [==============================] - 7s 555us/step - loss: 0.3206 - acc: 0.8603\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11983/11983 [==============================] - 6s 505us/step - loss: 0.3208 - acc: 0.8599\n",
      "Epoch 39/50\n",
      "11983/11983 [==============================] - 5s 439us/step - loss: 0.3175 - acc: 0.8611\n",
      "Epoch 40/50\n",
      "11983/11983 [==============================] - 5s 439us/step - loss: 0.3149 - acc: 0.8592\n",
      "Epoch 41/50\n",
      "11983/11983 [==============================] - 5s 448us/step - loss: 0.3146 - acc: 0.8610\n",
      "Epoch 42/50\n",
      "11983/11983 [==============================] - 5s 442us/step - loss: 0.3112 - acc: 0.8635\n",
      "Epoch 43/50\n",
      "11983/11983 [==============================] - 6s 502us/step - loss: 0.3081 - acc: 0.8666\n",
      "Epoch 44/50\n",
      "11983/11983 [==============================] - ETA: 0s - loss: 0.3085 - acc: 0.864 - 6s 466us/step - loss: 0.3086 - acc: 0.8643\n",
      "Epoch 45/50\n",
      "11983/11983 [==============================] - 6s 467us/step - loss: 0.3042 - acc: 0.8676\n",
      "Epoch 46/50\n",
      "11983/11983 [==============================] - 6s 473us/step - loss: 0.3035 - acc: 0.8676\n",
      "Epoch 47/50\n",
      "11983/11983 [==============================] - 7s 579us/step - loss: 0.3018 - acc: 0.8688\n",
      "Epoch 48/50\n",
      "11983/11983 [==============================] - 5s 458us/step - loss: 0.2983 - acc: 0.8681\n",
      "Epoch 49/50\n",
      "11983/11983 [==============================] - ETA: 0s - loss: 0.3016 - acc: 0.869 - 5s 455us/step - loss: 0.3014 - acc: 0.8698\n",
      "Epoch 50/50\n",
      "11983/11983 [==============================] - 6s 464us/step - loss: 0.2948 - acc: 0.8729\n",
      "1331/1331 [==============================] - 0s 155us/step\n",
      "Epoch 1/50\n",
      "11983/11983 [==============================] - 7s 584us/step - loss: 0.5837 - acc: 0.7421\n",
      "Epoch 2/50\n",
      "11983/11983 [==============================] - 6s 510us/step - loss: 0.4547 - acc: 0.7866\n",
      "Epoch 3/50\n",
      "11983/11983 [==============================] - 6s 511us/step - loss: 0.4317 - acc: 0.8066\n",
      "Epoch 4/50\n",
      "11983/11983 [==============================] - 6s 539us/step - loss: 0.4208 - acc: 0.8093\n",
      "Epoch 5/50\n",
      "11983/11983 [==============================] - 6s 510us/step - loss: 0.4143 - acc: 0.8157\n",
      "Epoch 6/50\n",
      "11983/11983 [==============================] - 6s 530us/step - loss: 0.4040 - acc: 0.8212\n",
      "Epoch 7/50\n",
      "11983/11983 [==============================] - 6s 519us/step - loss: 0.3964 - acc: 0.8198\n",
      "Epoch 8/50\n",
      "11983/11983 [==============================] - 7s 572us/step - loss: 0.3963 - acc: 0.8214\n",
      "Epoch 9/50\n",
      "11983/11983 [==============================] - 7s 585us/step - loss: 0.3938 - acc: 0.8227\n",
      "Epoch 10/50\n",
      "11983/11983 [==============================] - 7s 601us/step - loss: 0.3899 - acc: 0.8237 0s - loss: 0.3911 - acc: 0\n",
      "Epoch 11/50\n",
      "11983/11983 [==============================] - 8s 697us/step - loss: 0.3831 - acc: 0.8264\n",
      "Epoch 12/50\n",
      "11983/11983 [==============================] - 8s 635us/step - loss: 0.3858 - acc: 0.8246\n",
      "Epoch 13/50\n",
      "11983/11983 [==============================] - 8s 632us/step - loss: 0.3791 - acc: 0.8307\n",
      "Epoch 14/50\n",
      "11983/11983 [==============================] - 7s 578us/step - loss: 0.3759 - acc: 0.8309\n",
      "Epoch 15/50\n",
      "11983/11983 [==============================] - 7s 563us/step - loss: 0.3742 - acc: 0.8318\n",
      "Epoch 16/50\n",
      "11983/11983 [==============================] - 7s 613us/step - loss: 0.3705 - acc: 0.8338\n",
      "Epoch 17/50\n",
      "11983/11983 [==============================] - 6s 528us/step - loss: 0.3673 - acc: 0.8359\n",
      "Epoch 18/50\n",
      "11983/11983 [==============================] - 6s 533us/step - loss: 0.3631 - acc: 0.8362\n",
      "Epoch 19/50\n",
      "11983/11983 [==============================] - 6s 527us/step - loss: 0.3668 - acc: 0.8368\n",
      "Epoch 20/50\n",
      "11983/11983 [==============================] - 6s 495us/step - loss: 0.3593 - acc: 0.8404\n",
      "Epoch 21/50\n",
      "11983/11983 [==============================] - 6s 507us/step - loss: 0.3548 - acc: 0.8431\n",
      "Epoch 22/50\n",
      "11983/11983 [==============================] - 6s 475us/step - loss: 0.3548 - acc: 0.8432\n",
      "Epoch 23/50\n",
      "11983/11983 [==============================] - 7s 589us/step - loss: 0.3489 - acc: 0.8438\n",
      "Epoch 24/50\n",
      "11983/11983 [==============================] - 7s 607us/step - loss: 0.3518 - acc: 0.8467\n",
      "Epoch 25/50\n",
      "11983/11983 [==============================] - 6s 526us/step - loss: 0.3470 - acc: 0.8473\n",
      "Epoch 26/50\n",
      "11983/11983 [==============================] - 7s 557us/step - loss: 0.3456 - acc: 0.8480\n",
      "Epoch 27/50\n",
      "11983/11983 [==============================] - 6s 509us/step - loss: 0.3468 - acc: 0.8457\n",
      "Epoch 28/50\n",
      "11983/11983 [==============================] - 6s 466us/step - loss: 0.3400 - acc: 0.8508\n",
      "Epoch 29/50\n",
      "11983/11983 [==============================] - 6s 489us/step - loss: 0.3386 - acc: 0.8521\n",
      "Epoch 30/50\n",
      "11983/11983 [==============================] - 8s 633us/step - loss: 0.3352 - acc: 0.8530\n",
      "Epoch 31/50\n",
      "11983/11983 [==============================] - 6s 515us/step - loss: 0.3315 - acc: 0.8541\n",
      "Epoch 32/50\n",
      "11983/11983 [==============================] - 7s 553us/step - loss: 0.3334 - acc: 0.8522\n",
      "Epoch 33/50\n",
      "11983/11983 [==============================] - 7s 581us/step - loss: 0.3312 - acc: 0.8565\n",
      "Epoch 34/50\n",
      "11983/11983 [==============================] - 7s 611us/step - loss: 0.3260 - acc: 0.8569\n",
      "Epoch 35/50\n",
      "11983/11983 [==============================] - 7s 547us/step - loss: 0.3263 - acc: 0.8566\n",
      "Epoch 36/50\n",
      "11983/11983 [==============================] - 6s 528us/step - loss: 0.3249 - acc: 0.8575\n",
      "Epoch 37/50\n",
      "11983/11983 [==============================] - 7s 579us/step - loss: 0.3265 - acc: 0.8591\n",
      "Epoch 38/50\n",
      "11983/11983 [==============================] - 6s 512us/step - loss: 0.3246 - acc: 0.8576\n",
      "Epoch 39/50\n",
      "11983/11983 [==============================] - 8s 644us/step - loss: 0.3187 - acc: 0.8601\n",
      "Epoch 40/50\n",
      "11983/11983 [==============================] - 8s 639us/step - loss: 0.3167 - acc: 0.8599\n",
      "Epoch 41/50\n",
      "11983/11983 [==============================] - 7s 598us/step - loss: 0.3141 - acc: 0.8625\n",
      "Epoch 42/50\n",
      "11983/11983 [==============================] - 7s 560us/step - loss: 0.3132 - acc: 0.8615\n",
      "Epoch 43/50\n",
      "11983/11983 [==============================] - 6s 502us/step - loss: 0.3122 - acc: 0.8669\n",
      "Epoch 44/50\n",
      "11983/11983 [==============================] - 6s 464us/step - loss: 0.3069 - acc: 0.8673\n",
      "Epoch 45/50\n",
      "11983/11983 [==============================] - 6s 529us/step - loss: 0.3058 - acc: 0.8671\n",
      "Epoch 46/50\n",
      "11983/11983 [==============================] - 7s 610us/step - loss: 0.3083 - acc: 0.8661\n",
      "Epoch 47/50\n",
      "11983/11983 [==============================] - 5s 458us/step - loss: 0.3079 - acc: 0.8686\n",
      "Epoch 48/50\n",
      "11983/11983 [==============================] - 7s 549us/step - loss: 0.2994 - acc: 0.8688\n",
      "Epoch 49/50\n",
      "11983/11983 [==============================] - 7s 586us/step - loss: 0.2986 - acc: 0.8670\n",
      "Epoch 50/50\n",
      "11983/11983 [==============================] - 6s 541us/step - loss: 0.2999 - acc: 0.8699\n",
      "1331/1331 [==============================] - 0s 207us/step\n",
      "Epoch 1/50\n",
      "11983/11983 [==============================] - 6s 527us/step - loss: 0.5547 - acc: 0.7453\n",
      "Epoch 2/50\n",
      "11983/11983 [==============================] - 6s 480us/step - loss: 0.4463 - acc: 0.7930\n",
      "Epoch 3/50\n",
      "11983/11983 [==============================] - 5s 443us/step - loss: 0.4280 - acc: 0.8064\n",
      "Epoch 4/50\n",
      "11983/11983 [==============================] - 5s 458us/step - loss: 0.4166 - acc: 0.8095\n",
      "Epoch 5/50\n",
      "11983/11983 [==============================] - 6s 464us/step - loss: 0.4033 - acc: 0.8195\n",
      "Epoch 6/50\n",
      "11983/11983 [==============================] - 6s 492us/step - loss: 0.4013 - acc: 0.8202\n",
      "Epoch 7/50\n",
      "11983/11983 [==============================] - 6s 487us/step - loss: 0.3923 - acc: 0.8222\n",
      "Epoch 8/50\n",
      "11983/11983 [==============================] - 7s 559us/step - loss: 0.3942 - acc: 0.8182\n",
      "Epoch 9/50\n",
      "11983/11983 [==============================] - 7s 563us/step - loss: 0.3887 - acc: 0.8248\n",
      "Epoch 10/50\n",
      "11983/11983 [==============================] - 6s 509us/step - loss: 0.3850 - acc: 0.8273\n",
      "Epoch 11/50\n",
      "11983/11983 [==============================] - 6s 491us/step - loss: 0.3830 - acc: 0.8272\n",
      "Epoch 12/50\n",
      "11983/11983 [==============================] - 7s 590us/step - loss: 0.3798 - acc: 0.8300\n",
      "Epoch 13/50\n",
      "11983/11983 [==============================] - 7s 615us/step - loss: 0.3742 - acc: 0.8296\n",
      "Epoch 14/50\n",
      "11983/11983 [==============================] - 6s 525us/step - loss: 0.3692 - acc: 0.8338\n",
      "Epoch 15/50\n",
      "11983/11983 [==============================] - 6s 498us/step - loss: 0.3687 - acc: 0.8343\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11983/11983 [==============================] - 8s 637us/step - loss: 0.3674 - acc: 0.8368\n",
      "Epoch 17/50\n",
      "11983/11983 [==============================] - 7s 558us/step - loss: 0.3634 - acc: 0.8370\n",
      "Epoch 18/50\n",
      "11983/11983 [==============================] - 5s 450us/step - loss: 0.3631 - acc: 0.8366\n",
      "Epoch 19/50\n",
      "11983/11983 [==============================] - 5s 437us/step - loss: 0.3599 - acc: 0.8389\n",
      "Epoch 20/50\n",
      "11983/11983 [==============================] - 6s 480us/step - loss: 0.3586 - acc: 0.8427\n",
      "Epoch 21/50\n",
      "11983/11983 [==============================] - 7s 623us/step - loss: 0.3563 - acc: 0.8413\n",
      "Epoch 22/50\n",
      "11983/11983 [==============================] - 7s 584us/step - loss: 0.3541 - acc: 0.8413\n",
      "Epoch 23/50\n",
      "11983/11983 [==============================] - 6s 525us/step - loss: 0.3505 - acc: 0.8448\n",
      "Epoch 24/50\n",
      "11983/11983 [==============================] - 6s 500us/step - loss: 0.3455 - acc: 0.8470\n",
      "Epoch 25/50\n",
      "11983/11983 [==============================] - 5s 458us/step - loss: 0.3423 - acc: 0.8508\n",
      "Epoch 26/50\n",
      "11983/11983 [==============================] - 5s 458us/step - loss: 0.3422 - acc: 0.8462\n",
      "Epoch 27/50\n",
      "11983/11983 [==============================] - 6s 462us/step - loss: 0.3392 - acc: 0.8486\n",
      "Epoch 28/50\n",
      "11983/11983 [==============================] - 5s 456us/step - loss: 0.3355 - acc: 0.8525\n",
      "Epoch 29/50\n",
      "11983/11983 [==============================] - 5s 442us/step - loss: 0.3357 - acc: 0.8515\n",
      "Epoch 30/50\n",
      "11983/11983 [==============================] - 6s 516us/step - loss: 0.3315 - acc: 0.8540\n",
      "Epoch 31/50\n",
      "11983/11983 [==============================] - 6s 499us/step - loss: 0.3327 - acc: 0.8534\n",
      "Epoch 32/50\n",
      "11983/11983 [==============================] - 6s 475us/step - loss: 0.3276 - acc: 0.8536\n",
      "Epoch 33/50\n",
      "11983/11983 [==============================] - 6s 497us/step - loss: 0.3252 - acc: 0.8564\n",
      "Epoch 34/50\n",
      "11983/11983 [==============================] - 6s 518us/step - loss: 0.3215 - acc: 0.8591\n",
      "Epoch 35/50\n",
      "11983/11983 [==============================] - ETA: 0s - loss: 0.3219 - acc: 0.860 - 7s 555us/step - loss: 0.3220 - acc: 0.8600\n",
      "Epoch 36/50\n",
      "11983/11983 [==============================] - 6s 478us/step - loss: 0.3188 - acc: 0.8601\n",
      "Epoch 37/50\n",
      "11983/11983 [==============================] - 6s 508us/step - loss: 0.3158 - acc: 0.8604\n",
      "Epoch 38/50\n",
      "11983/11983 [==============================] - 6s 521us/step - loss: 0.3138 - acc: 0.8626\n",
      "Epoch 39/50\n",
      "11983/11983 [==============================] - 6s 521us/step - loss: 0.3144 - acc: 0.8621\n",
      "Epoch 40/50\n",
      "11983/11983 [==============================] - 7s 562us/step - loss: 0.3119 - acc: 0.8617\n",
      "Epoch 41/50\n",
      "11983/11983 [==============================] - 6s 529us/step - loss: 0.3093 - acc: 0.8644\n",
      "Epoch 42/50\n",
      "11983/11983 [==============================] - 6s 526us/step - loss: 0.3056 - acc: 0.8656\n",
      "Epoch 43/50\n",
      "11983/11983 [==============================] - 6s 497us/step - loss: 0.3031 - acc: 0.8651\n",
      "Epoch 44/50\n",
      "11983/11983 [==============================] - 6s 472us/step - loss: 0.3011 - acc: 0.8670\n",
      "Epoch 45/50\n",
      "11983/11983 [==============================] - 6s 514us/step - loss: 0.2990 - acc: 0.8666\n",
      "Epoch 46/50\n",
      "11983/11983 [==============================] - 6s 489us/step - loss: 0.2991 - acc: 0.8676\n",
      "Epoch 47/50\n",
      "11983/11983 [==============================] - 6s 464us/step - loss: 0.2950 - acc: 0.8699\n",
      "Epoch 48/50\n",
      "11983/11983 [==============================] - 6s 496us/step - loss: 0.2921 - acc: 0.8707\n",
      "Epoch 49/50\n",
      "11983/11983 [==============================] - 7s 610us/step - loss: 0.2963 - acc: 0.8701\n",
      "Epoch 50/50\n",
      "11983/11983 [==============================] - 6s 486us/step - loss: 0.2912 - acc: 0.8762\n",
      "1331/1331 [==============================] - 0s 169us/step\n",
      "Epoch 1/50\n",
      "11983/11983 [==============================] - ETA: 0s - loss: 0.5365 - acc: 0.752 - 6s 518us/step - loss: 0.5364 - acc: 0.7521\n",
      "Epoch 2/50\n",
      "11983/11983 [==============================] - 8s 628us/step - loss: 0.4549 - acc: 0.7878\n",
      "Epoch 3/50\n",
      "11983/11983 [==============================] - 6s 480us/step - loss: 0.4294 - acc: 0.8036\n",
      "Epoch 4/50\n",
      "11983/11983 [==============================] - 6s 503us/step - loss: 0.4123 - acc: 0.8146\n",
      "Epoch 5/50\n",
      "11983/11983 [==============================] - ETA: 0s - loss: 0.4098 - acc: 0.813 - 6s 535us/step - loss: 0.4098 - acc: 0.8136\n",
      "Epoch 6/50\n",
      "11983/11983 [==============================] - 6s 479us/step - loss: 0.4029 - acc: 0.8158\n",
      "Epoch 7/50\n",
      "11983/11983 [==============================] - 6s 475us/step - loss: 0.4002 - acc: 0.8191\n",
      "Epoch 8/50\n",
      "11983/11983 [==============================] - 6s 467us/step - loss: 0.3912 - acc: 0.8218\n",
      "Epoch 9/50\n",
      "11983/11983 [==============================] - 6s 541us/step - loss: 0.3910 - acc: 0.8221\n",
      "Epoch 10/50\n",
      "11983/11983 [==============================] - 6s 515us/step - loss: 0.3867 - acc: 0.8253\n",
      "Epoch 11/50\n",
      "11983/11983 [==============================] - 6s 508us/step - loss: 0.3834 - acc: 0.8258\n",
      "Epoch 12/50\n",
      "11983/11983 [==============================] - 6s 519us/step - loss: 0.3807 - acc: 0.8309\n",
      "Epoch 13/50\n",
      "11983/11983 [==============================] - 6s 488us/step - loss: 0.3775 - acc: 0.8336\n",
      "Epoch 14/50\n",
      "11983/11983 [==============================] - 7s 568us/step - loss: 0.3750 - acc: 0.8348\n",
      "Epoch 15/50\n",
      "11983/11983 [==============================] - 7s 547us/step - loss: 0.3703 - acc: 0.8332\n",
      "Epoch 16/50\n",
      "11983/11983 [==============================] - 7s 569us/step - loss: 0.3688 - acc: 0.8323\n",
      "Epoch 17/50\n",
      "11983/11983 [==============================] - 7s 616us/step - loss: 0.3680 - acc: 0.8371\n",
      "Epoch 18/50\n",
      "11983/11983 [==============================] - 6s 472us/step - loss: 0.3643 - acc: 0.8389\n",
      "Epoch 19/50\n",
      "11983/11983 [==============================] - 6s 496us/step - loss: 0.3603 - acc: 0.8406\n",
      "Epoch 20/50\n",
      "11983/11983 [==============================] - 6s 525us/step - loss: 0.3576 - acc: 0.8413\n",
      "Epoch 21/50\n",
      "11983/11983 [==============================] - 6s 521us/step - loss: 0.3578 - acc: 0.8424\n",
      "Epoch 22/50\n",
      "11983/11983 [==============================] - 7s 551us/step - loss: 0.3516 - acc: 0.8415\n",
      "Epoch 23/50\n",
      "11983/11983 [==============================] - 6s 525us/step - loss: 0.3489 - acc: 0.8458 1s - loss: \n",
      "Epoch 24/50\n",
      "11983/11983 [==============================] - 7s 559us/step - loss: 0.3482 - acc: 0.8454\n",
      "Epoch 25/50\n",
      "11983/11983 [==============================] - 9s 743us/step - loss: 0.3450 - acc: 0.8474\n",
      "Epoch 26/50\n",
      "11983/11983 [==============================] - 10s 854us/step - loss: 0.3441 - acc: 0.8471\n",
      "Epoch 27/50\n",
      "11983/11983 [==============================] - 7s 553us/step - loss: 0.3391 - acc: 0.8483\n",
      "Epoch 28/50\n",
      "11983/11983 [==============================] - 7s 561us/step - loss: 0.3381 - acc: 0.8519\n",
      "Epoch 29/50\n",
      "11983/11983 [==============================] - 6s 499us/step - loss: 0.3363 - acc: 0.8504\n",
      "Epoch 30/50\n",
      "11983/11983 [==============================] - 6s 542us/step - loss: 0.3345 - acc: 0.8517\n",
      "Epoch 31/50\n",
      "11983/11983 [==============================] - 6s 494us/step - loss: 0.3347 - acc: 0.8521\n",
      "Epoch 32/50\n",
      "11983/11983 [==============================] - 7s 576us/step - loss: 0.3289 - acc: 0.8546\n",
      "Epoch 33/50\n",
      "11983/11983 [==============================] - 7s 581us/step - loss: 0.3258 - acc: 0.8555\n",
      "Epoch 34/50\n",
      "11983/11983 [==============================] - 7s 579us/step - loss: 0.3254 - acc: 0.8561\n",
      "Epoch 35/50\n",
      "11983/11983 [==============================] - 7s 621us/step - loss: 0.3207 - acc: 0.8603\n",
      "Epoch 36/50\n",
      "11983/11983 [==============================] - 8s 707us/step - loss: 0.3219 - acc: 0.8556\n",
      "Epoch 37/50\n",
      "11983/11983 [==============================] - 7s 604us/step - loss: 0.3199 - acc: 0.8591\n",
      "Epoch 38/50\n",
      "11983/11983 [==============================] - 7s 563us/step - loss: 0.3163 - acc: 0.8596\n",
      "Epoch 39/50\n",
      "11983/11983 [==============================] - 6s 476us/step - loss: 0.3162 - acc: 0.8648\n",
      "Epoch 40/50\n",
      "11983/11983 [==============================] - 6s 523us/step - loss: 0.3139 - acc: 0.8585\n",
      "Epoch 41/50\n",
      "11983/11983 [==============================] - 8s 679us/step - loss: 0.3104 - acc: 0.8641\n",
      "Epoch 42/50\n",
      "11983/11983 [==============================] - 7s 611us/step - loss: 0.3059 - acc: 0.8693\n",
      "Epoch 43/50\n",
      "11983/11983 [==============================] - 5s 441us/step - loss: 0.3087 - acc: 0.8656\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11983/11983 [==============================] - 6s 528us/step - loss: 0.3060 - acc: 0.8645\n",
      "Epoch 45/50\n",
      "11983/11983 [==============================] - 7s 622us/step - loss: 0.3017 - acc: 0.8671\n",
      "Epoch 46/50\n",
      "11983/11983 [==============================] - 7s 552us/step - loss: 0.2956 - acc: 0.8684\n",
      "Epoch 47/50\n",
      "11983/11983 [==============================] - 7s 565us/step - loss: 0.2983 - acc: 0.8696\n",
      "Epoch 48/50\n",
      "11983/11983 [==============================] - 8s 638us/step - loss: 0.2929 - acc: 0.8688\n",
      "Epoch 49/50\n",
      "11983/11983 [==============================] - 8s 627us/step - loss: 0.2943 - acc: 0.8707 0s - loss: 0.2937 - acc: 0.\n",
      "Epoch 50/50\n",
      "11983/11983 [==============================] - 6s 494us/step - loss: 0.2889 - acc: 0.8722\n",
      "1331/1331 [==============================] - 0s 195us/step\n",
      "Epoch 1/50\n",
      "11983/11983 [==============================] - 7s 617us/step - loss: 0.6273 - acc: 0.7481\n",
      "Epoch 2/50\n",
      "11983/11983 [==============================] - 8s 658us/step - loss: 0.4558 - acc: 0.7915\n",
      "Epoch 3/50\n",
      "11983/11983 [==============================] - 7s 562us/step - loss: 0.4298 - acc: 0.8055\n",
      "Epoch 4/50\n",
      "11983/11983 [==============================] - 6s 531us/step - loss: 0.4177 - acc: 0.8093\n",
      "Epoch 5/50\n",
      "11983/11983 [==============================] - 6s 536us/step - loss: 0.4085 - acc: 0.8169\n",
      "Epoch 6/50\n",
      "11983/11983 [==============================] - 6s 488us/step - loss: 0.4073 - acc: 0.8195\n",
      "Epoch 7/50\n",
      "11983/11983 [==============================] - 6s 464us/step - loss: 0.4008 - acc: 0.8211\n",
      "Epoch 8/50\n",
      "11983/11983 [==============================] - 6s 489us/step - loss: 0.3932 - acc: 0.8202\n",
      "Epoch 9/50\n",
      "11983/11983 [==============================] - 6s 507us/step - loss: 0.3926 - acc: 0.8227\n",
      "Epoch 10/50\n",
      "11983/11983 [==============================] - 6s 485us/step - loss: 0.3917 - acc: 0.8243\n",
      "Epoch 11/50\n",
      "11983/11983 [==============================] - 6s 486us/step - loss: 0.3874 - acc: 0.8269\n",
      "Epoch 12/50\n",
      "11983/11983 [==============================] - 7s 554us/step - loss: 0.3795 - acc: 0.8282\n",
      "Epoch 13/50\n",
      "11983/11983 [==============================] - 7s 558us/step - loss: 0.3805 - acc: 0.8316\n",
      "Epoch 14/50\n",
      "11983/11983 [==============================] - 6s 538us/step - loss: 0.3766 - acc: 0.8313\n",
      "Epoch 15/50\n",
      "11983/11983 [==============================] - 6s 490us/step - loss: 0.3725 - acc: 0.8316\n",
      "Epoch 16/50\n",
      "11983/11983 [==============================] - 6s 475us/step - loss: 0.3711 - acc: 0.8371\n",
      "Epoch 17/50\n",
      "11983/11983 [==============================] - 6s 509us/step - loss: 0.3673 - acc: 0.8343\n",
      "Epoch 18/50\n",
      "11983/11983 [==============================] - 6s 501us/step - loss: 0.3619 - acc: 0.8409\n",
      "Epoch 19/50\n",
      "11983/11983 [==============================] - 6s 460us/step - loss: 0.3640 - acc: 0.8372\n",
      "Epoch 20/50\n",
      "11983/11983 [==============================] - 6s 491us/step - loss: 0.3610 - acc: 0.8409\n",
      "Epoch 21/50\n",
      "11983/11983 [==============================] - 6s 500us/step - loss: 0.3559 - acc: 0.8434\n",
      "Epoch 22/50\n",
      "11983/11983 [==============================] - 7s 551us/step - loss: 0.3562 - acc: 0.8460\n",
      "Epoch 23/50\n",
      "11983/11983 [==============================] - 8s 632us/step - loss: 0.3551 - acc: 0.8420\n",
      "Epoch 24/50\n",
      "11983/11983 [==============================] - 9s 755us/step - loss: 0.3548 - acc: 0.8434\n",
      "Epoch 25/50\n",
      "11983/11983 [==============================] - 8s 707us/step - loss: 0.3508 - acc: 0.8452\n",
      "Epoch 26/50\n",
      "11983/11983 [==============================] - 7s 584us/step - loss: 0.3476 - acc: 0.8450\n",
      "Epoch 27/50\n",
      "11983/11983 [==============================] - 7s 553us/step - loss: 0.3473 - acc: 0.8469\n",
      "Epoch 28/50\n",
      "11983/11983 [==============================] - 6s 487us/step - loss: 0.3415 - acc: 0.8481\n",
      "Epoch 29/50\n",
      "11983/11983 [==============================] - 6s 497us/step - loss: 0.3402 - acc: 0.8510\n",
      "Epoch 30/50\n",
      "11983/11983 [==============================] - 7s 567us/step - loss: 0.3397 - acc: 0.8510\n",
      "Epoch 31/50\n",
      "11983/11983 [==============================] - 6s 522us/step - loss: 0.3387 - acc: 0.8515\n",
      "Epoch 32/50\n",
      "11983/11983 [==============================] - 6s 480us/step - loss: 0.3313 - acc: 0.8516\n",
      "Epoch 33/50\n",
      "11983/11983 [==============================] - 6s 518us/step - loss: 0.3290 - acc: 0.8548\n",
      "Epoch 34/50\n",
      "11983/11983 [==============================] - 7s 566us/step - loss: 0.3299 - acc: 0.8542\n",
      "Epoch 35/50\n",
      "11983/11983 [==============================] - 6s 500us/step - loss: 0.3252 - acc: 0.8558\n",
      "Epoch 36/50\n",
      "11983/11983 [==============================] - 6s 509us/step - loss: 0.3246 - acc: 0.8560\n",
      "Epoch 37/50\n",
      "11983/11983 [==============================] - 7s 549us/step - loss: 0.3208 - acc: 0.8596\n",
      "Epoch 38/50\n",
      "11983/11983 [==============================] - 7s 554us/step - loss: 0.3208 - acc: 0.8582\n",
      "Epoch 39/50\n",
      "11983/11983 [==============================] - 6s 519us/step - loss: 0.3193 - acc: 0.8611\n",
      "Epoch 40/50\n",
      "11983/11983 [==============================] - 6s 479us/step - loss: 0.3165 - acc: 0.8578\n",
      "Epoch 41/50\n",
      "11983/11983 [==============================] - 6s 516us/step - loss: 0.3126 - acc: 0.8611\n",
      "Epoch 42/50\n",
      "11983/11983 [==============================] - 6s 480us/step - loss: 0.3112 - acc: 0.8630\n",
      "Epoch 43/50\n",
      "11983/11983 [==============================] - 6s 489us/step - loss: 0.3121 - acc: 0.8643\n",
      "Epoch 44/50\n",
      "11983/11983 [==============================] - 6s 498us/step - loss: 0.3068 - acc: 0.8629\n",
      "Epoch 45/50\n",
      "11983/11983 [==============================] - 6s 481us/step - loss: 0.3072 - acc: 0.8646\n",
      "Epoch 46/50\n",
      "11983/11983 [==============================] - 6s 492us/step - loss: 0.3057 - acc: 0.8661\n",
      "Epoch 47/50\n",
      "11983/11983 [==============================] - 6s 492us/step - loss: 0.2986 - acc: 0.8701\n",
      "Epoch 48/50\n",
      "11983/11983 [==============================] - 6s 518us/step - loss: 0.2944 - acc: 0.8709\n",
      "Epoch 49/50\n",
      "11983/11983 [==============================] - 6s 490us/step - loss: 0.2951 - acc: 0.8683\n",
      "Epoch 50/50\n",
      "11983/11983 [==============================] - 6s 472us/step - loss: 0.2934 - acc: 0.8702\n",
      "1331/1331 [==============================] - 0s 199us/step\n",
      "Epoch 1/50\n",
      "11983/11983 [==============================] - 6s 495us/step - loss: 0.6014 - acc: 0.7435\n",
      "Epoch 2/50\n",
      "11983/11983 [==============================] - 6s 503us/step - loss: 0.4473 - acc: 0.7945\n",
      "Epoch 3/50\n",
      "11983/11983 [==============================] - 6s 476us/step - loss: 0.4309 - acc: 0.8051\n",
      "Epoch 4/50\n",
      "11983/11983 [==============================] - 5s 448us/step - loss: 0.4192 - acc: 0.8109\n",
      "Epoch 5/50\n",
      "11983/11983 [==============================] - 6s 460us/step - loss: 0.4078 - acc: 0.8159\n",
      "Epoch 6/50\n",
      "11983/11983 [==============================] - 5s 450us/step - loss: 0.4025 - acc: 0.8161\n",
      "Epoch 7/50\n",
      "11983/11983 [==============================] - 5s 457us/step - loss: 0.3999 - acc: 0.8181\n",
      "Epoch 8/50\n",
      "11983/11983 [==============================] - 6s 526us/step - loss: 0.3923 - acc: 0.8220\n",
      "Epoch 9/50\n",
      "11983/11983 [==============================] - 6s 468us/step - loss: 0.3920 - acc: 0.8188\n",
      "Epoch 10/50\n",
      "11983/11983 [==============================] - 6s 496us/step - loss: 0.3855 - acc: 0.8243\n",
      "Epoch 11/50\n",
      "11983/11983 [==============================] - 6s 499us/step - loss: 0.3880 - acc: 0.8281\n",
      "Epoch 12/50\n",
      "11983/11983 [==============================] - 6s 512us/step - loss: 0.3801 - acc: 0.8303\n",
      "Epoch 13/50\n",
      "11983/11983 [==============================] - 6s 493us/step - loss: 0.3802 - acc: 0.8302\n",
      "Epoch 14/50\n",
      "11983/11983 [==============================] - 6s 485us/step - loss: 0.3760 - acc: 0.8314\n",
      "Epoch 15/50\n",
      "11983/11983 [==============================] - 8s 690us/step - loss: 0.3706 - acc: 0.8365\n",
      "Epoch 16/50\n",
      "11983/11983 [==============================] - 8s 691us/step - loss: 0.3673 - acc: 0.8388\n",
      "Epoch 17/50\n",
      "11983/11983 [==============================] - 7s 580us/step - loss: 0.3656 - acc: 0.8369\n",
      "Epoch 18/50\n",
      "11983/11983 [==============================] - 7s 611us/step - loss: 0.3617 - acc: 0.8399\n",
      "Epoch 19/50\n",
      "11983/11983 [==============================] - 6s 506us/step - loss: 0.3617 - acc: 0.8385\n",
      "Epoch 20/50\n",
      "11983/11983 [==============================] - 6s 502us/step - loss: 0.3609 - acc: 0.8384\n",
      "Epoch 21/50\n",
      "11983/11983 [==============================] - 7s 551us/step - loss: 0.3560 - acc: 0.8443\n",
      "Epoch 22/50\n",
      "11983/11983 [==============================] - 7s 553us/step - loss: 0.3523 - acc: 0.8439\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11983/11983 [==============================] - 6s 495us/step - loss: 0.3481 - acc: 0.8456\n",
      "Epoch 24/50\n",
      "11983/11983 [==============================] - 6s 500us/step - loss: 0.3494 - acc: 0.8439\n",
      "Epoch 25/50\n",
      "11983/11983 [==============================] - 6s 473us/step - loss: 0.3460 - acc: 0.8502\n",
      "Epoch 26/50\n",
      "11983/11983 [==============================] - 6s 484us/step - loss: 0.3441 - acc: 0.8485\n",
      "Epoch 27/50\n",
      "11983/11983 [==============================] - 6s 470us/step - loss: 0.3419 - acc: 0.8498\n",
      "Epoch 28/50\n",
      "11983/11983 [==============================] - 5s 431us/step - loss: 0.3355 - acc: 0.8520\n",
      "Epoch 29/50\n",
      "11983/11983 [==============================] - 5s 421us/step - loss: 0.3354 - acc: 0.8536\n",
      "Epoch 30/50\n",
      "11983/11983 [==============================] - 6s 472us/step - loss: 0.3330 - acc: 0.8515\n",
      "Epoch 31/50\n",
      "11983/11983 [==============================] - 9s 778us/step - loss: 0.3308 - acc: 0.8557\n",
      "Epoch 32/50\n",
      "11983/11983 [==============================] - 7s 557us/step - loss: 0.3301 - acc: 0.8540\n",
      "Epoch 33/50\n",
      "11983/11983 [==============================] - 6s 517us/step - loss: 0.3300 - acc: 0.8575\n",
      "Epoch 34/50\n",
      "11983/11983 [==============================] - 7s 581us/step - loss: 0.3263 - acc: 0.8574\n",
      "Epoch 35/50\n",
      "11983/11983 [==============================] - 6s 510us/step - loss: 0.3264 - acc: 0.8538\n",
      "Epoch 36/50\n",
      "11983/11983 [==============================] - 6s 541us/step - loss: 0.3237 - acc: 0.8585\n",
      "Epoch 37/50\n",
      "11983/11983 [==============================] - 7s 567us/step - loss: 0.3212 - acc: 0.8580\n",
      "Epoch 38/50\n",
      "11983/11983 [==============================] - 7s 574us/step - loss: 0.3191 - acc: 0.8589\n",
      "Epoch 39/50\n",
      "11983/11983 [==============================] - 7s 573us/step - loss: 0.3116 - acc: 0.8645\n",
      "Epoch 40/50\n",
      "11983/11983 [==============================] - 6s 517us/step - loss: 0.3185 - acc: 0.8589\n",
      "Epoch 41/50\n",
      "11983/11983 [==============================] - 8s 666us/step - loss: 0.3128 - acc: 0.8616\n",
      "Epoch 42/50\n",
      "11983/11983 [==============================] - 6s 499us/step - loss: 0.3105 - acc: 0.8644\n",
      "Epoch 43/50\n",
      "11983/11983 [==============================] - 7s 549us/step - loss: 0.3099 - acc: 0.8632\n",
      "Epoch 44/50\n",
      "11983/11983 [==============================] - 6s 512us/step - loss: 0.3051 - acc: 0.8639\n",
      "Epoch 45/50\n",
      "11983/11983 [==============================] - 6s 506us/step - loss: 0.3034 - acc: 0.8701\n",
      "Epoch 46/50\n",
      "11983/11983 [==============================] - 6s 538us/step - loss: 0.3013 - acc: 0.8676\n",
      "Epoch 47/50\n",
      "11983/11983 [==============================] - 6s 502us/step - loss: 0.2978 - acc: 0.8671\n",
      "Epoch 48/50\n",
      "11983/11983 [==============================] - 7s 554us/step - loss: 0.2983 - acc: 0.8691\n",
      "Epoch 49/50\n",
      "11983/11983 [==============================] - 6s 473us/step - loss: 0.2962 - acc: 0.8695\n",
      "Epoch 50/50\n",
      "11983/11983 [==============================] - 6s 473us/step - loss: 0.2946 - acc: 0.8682\n",
      "1331/1331 [==============================] - 0s 209us/step\n",
      "Accuracy mean: 0.840095504972\n",
      "Accuracy variance: 0.00808718825652\n"
     ]
    }
   ],
   "source": [
    "# estimators \n",
    "# Evaluating the ANN\n",
    "t0 = time()\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential # initialize neural network library\n",
    "from keras.layers import Dense # build our layers library\n",
    "def build_classifier():\n",
    "    classifier = Sequential() # initialize neural network\n",
    "    classifier.add(Dense(units = 1024, kernel_initializer = 'uniform', activation = 'relu', input_dim = X_train.shape[1]))\n",
    "    classifier.add(Dense(units = 512, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier\n",
    "classifier = KerasClassifier(build_fn = build_classifier, epochs = 50)\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "mean = accuracies.mean()\n",
    "variance = accuracies.std()\n",
    "print(\"Accuracy mean: \"+ str(mean))\n",
    "print(\"Accuracy variance: \"+ str(variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' Time ', '3141.778', ' seconds')\n",
      "[[1566  451]\n",
      " [ 207 3482]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.88      0.78      0.83      2017\n",
      "        1.0       0.89      0.94      0.91      3689\n",
      "\n",
      "avg / total       0.88      0.88      0.88      5706\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tt = time() - t0     \n",
    "print (\" Time \",format(round(tt,3)),\" seconds\")\n",
    "# Matrix de confusion\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of DecisionTreeClassifier = 81.090081 %\n",
      "[[1471  546]\n",
      " [ 533 3156]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.73      0.73      0.73      2017\n",
      "        1.0       0.85      0.86      0.85      3689\n",
      "\n",
      "avg / total       0.81      0.81      0.81      5706\n",
      "\n",
      "(' Time ', '0.295', ' seconds')\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy of DecisionTreeClassifier = {:.6f} %\".format(acc * 100))\n",
    "# Matrix de confusion\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,y_pred))\n",
    "tt = time() - t0     \n",
    "print (\" Time \",format(round(tt,3)),\" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of RandomForestClassifier = 88.135296 %\n",
      "[[1560  457]\n",
      " [ 220 3469]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.88      0.77      0.82      2017\n",
      "        1.0       0.88      0.94      0.91      3689\n",
      "\n",
      "avg / total       0.88      0.88      0.88      5706\n",
      "\n",
      "(' Time ', '5.354', ' seconds')\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy of RandomForestClassifier = {:.6f} %\".format(acc * 100))\n",
    "# Matrix de confusion\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,y_pred))\n",
    "tt = time() - t0     \n",
    "print (\" Time \",format(round(tt,3)),\" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of ExtraTreesClassifier = 87.644585 %\n",
      "[[1500  517]\n",
      " [ 188 3501]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.89      0.74      0.81      2017\n",
      "        1.0       0.87      0.95      0.91      3689\n",
      "\n",
      "avg / total       0.88      0.88      0.87      5706\n",
      "\n",
      "(' Time ', '1.661', ' seconds')\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf = ExtraTreesClassifier(n_estimators=100)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy of ExtraTreesClassifier = {:.6f} %\".format(acc * 100))\n",
    "# Matrix de confusion\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,y_pred))\n",
    "tt = time() - t0     \n",
    "print (\" Time \",format(round(tt,3)),\" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of AdaBoostClassifier = 84.086926 %\n",
      "[[1442  575]\n",
      " [ 333 3356]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.81      0.71      0.76      2017\n",
      "        1.0       0.85      0.91      0.88      3689\n",
      "\n",
      "avg / total       0.84      0.84      0.84      5706\n",
      "\n",
      "(' Time ', '2.65', ' seconds')\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy of AdaBoostClassifier = {:.6f} %\".format(acc * 100))\n",
    "# Matrix de confusion\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,y_pred))\n",
    "tt = time() - t0     \n",
    "print (\" Time \",format(round(tt,3)),\" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Naive_bayes  GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of GaussianNB = 72.450053 %\n",
      "[[ 741 1276]\n",
      " [ 296 3393]]\n",
      "\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.71      0.37      0.49      2017\n",
      "        1.0       0.73      0.92      0.81      3689\n",
      "\n",
      "avg / total       0.72      0.72      0.70      5706\n",
      "\n",
      "(' Time ', '0.169', ' seconds')\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb = gnb.fit(X_train, y_train)\n",
    "\n",
    "y_pred= gnb.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy of GaussianNB = {:.6f} %\".format(acc * 100))\n",
    "# Matrix de confusion\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print('\\n')\n",
    "print(classification_report(y_test,y_pred))\n",
    "tt = time() - t0     \n",
    "print (\" Time \",format(round(tt,3)),\" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
